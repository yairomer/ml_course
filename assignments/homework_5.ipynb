{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 5 - Logistic Regression, MLP, and CNN with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will solve the face recognition problem again. This time we will be using a multilayer perceptron (MLP) and a convolutional neural network (CNN). We will do so usign the PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminders\n",
    "\n",
    "- Start by making a copy of this notebook in order to be able to save it.\n",
    "- Use **Ctrl+[** to expend all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip of the day - Progress Bar\n",
    "\n",
    "When running a long calculation, we would usually want to have a progress bar to track the progress of our process. One great python package for creating such a progress bar is [**tqdm**](https://github.com/tqdm/tqdm). This package is easy to use and offers a highly customizable progress bar. \n",
    "\n",
    "For example, to add a progress bar to an existing loop, simply surrounding the iterable which the loops run over with the **tqdm_notebook** command:\n",
    "\n",
    "```python\n",
    "import tqdm\n",
    "for x in tqdm.tqdm_notebook(some_list):\n",
    "    some_long_running_function(x)\n",
    "```\n",
    "\n",
    "✍️ Add a progress bar to the following loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "for i in range(10):\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    print('Step {}'.format(i))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your IDs\n",
    "\n",
    "✍️ Fill in your IDs in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "# Replace the IDs bellow with our own\n",
    "student1_id = '012345678'\n",
    "student2_id = '012345678'\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('Hello ' + student1_id + ' & ' + student2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "Importing the NumPy, Pandas and Matplotlib packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## This line makes matplotlib plot the figures inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "## Set some default values of the the matplotlib plots\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
    "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Processing Unit (GPU)\n",
    "\n",
    "GPUs are special processing cards which were originally developed to for accelerating graphical related calculations such as 3D rendering and adujsting and playing a high-resolution video. Today these cards are also in use for a variety of tasks which are not necessarily graphic related, such as training neural networks.\n",
    "\n",
    "These GPUs are optimal for parallelizing simple operations on a large amount of data. The CPU (Central Processing Unit) is the computer's main processing unit and usually has a few fast and \"strong\" processing components called cores  (usually up to tens of cores ). As opposed to it,  a GPUs usually has many (usually thousands of) slower and \"weaker\" cores.\n",
    "\n",
    "When running a process which performs some mathematical operation to a large amount of data, for example calculating the $e^x$ for each element in a large matrix or multiplication between two matrices, we can speed up our process significantly by running it on a GPU.\n",
    "\n",
    "The GPU does not share the same memory space with the CPU and has its own memory. Therefore before performing any calculation using a GPU, we must first transfer our data to the GPU's memory.\n",
    "\n",
    "### Colab and GPUs\n",
    "\n",
    "In this assignment, we will train our network on a GPU. Colab offers free GPU support, but it is not enabled by default. To enable it, go to the menu bar, open **Runtime->Change runtime type** and change **hardware accelerator** to GPU. Click **save** to save your selection. You will see how we can tell our code to perform an operation on the GPU instead of on the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "PyTorch is a framework (a collection of tools) which significantly simplify the process of building and training neural networks. This framework was initially developed and is currently backed by Facebook. PyTorch is only one of many great such frameworks which currently exist. For a list of some of the commonly used frameworks today, see workshop 9.\n",
    "\n",
    "Specifically, in this assignment, we will relay on PyTorch for the following features:\n",
    "- The package's ability to automatic calculate gradients (using back-propagation)\n",
    "- The package's ability to move a variable to the GPU and perform calculations on it.\n",
    "- The package's stochastic gradient descent optimization object.\n",
    "- The built-in objects/function for building and training models:\n",
    "    - Linear layer\n",
    "    - Convolutional layers\n",
    "    - Relu\n",
    "    - SoftMax\n",
    "    - Minus-logLikelihood loss\n",
    " \n",
    "In this homework assignment, we will **not** cover all of what is needed for using PyTorch. It is aimed to show you the basics idea of what the framework has to offer. To better understand PyTorch, a good place to start are the great tutorials on the package's [website](https://pytorch.org/tutorials/index.html). The \"60 Minute Blitz\" along with the \"Learning PyTorch with Examples\" on the website provide a great starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "The basic PyTorch object is the tensor with has a very similar (but not exact) interface to that of the NumPy array. A few differences which are worth mentioning:\n",
    "- Tensors do not yet support the \"@\" operator of performing matrix multiplication. It is performed by using  **torch.matmul(a_mat, b_mat)**.\n",
    "- The transpose of a matrix is given by  **a_mat.t()** (instead of the **a_mat.T** method which is in use in numpy)\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  ## importing PyTorch\n",
    "\n",
    "## Defining a tensor from lists of numbers\n",
    "x1 = torch.tensor([[1.,2.], [3., 4.]])\n",
    "print('x1=\\n{}\\n'.format(x1))\n",
    "\n",
    "## Creating a random tensor\n",
    "x2 = torch.randint(low=0, high=10, size=(2, 3)).float()\n",
    "print('x2=\\n{}\\n'.format(x1))\n",
    "\n",
    "## Multipliing tensors\n",
    "y = torch.matmul(x1.t(), x2)\n",
    "print('torch.matmul(x1.t(), x2)=\\n{}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Important Comment About Single & Double Precision & Fixed Points\n",
    "\n",
    "By default, numpy uses 64 bit to store floating point numbers. This representation is optimal for most CPUs. In contrast to that, PyTorch uses 32 bits, which is optimal for most GPUs. The 64-bit representation is called **double precision**, and the 32-bit is called **single precision**.\n",
    "\n",
    "Most of PyTorch's operations can only be performed only between two tensors of the same type. Therefore we will make sure that all of our tensors will be stored using single precision. You can convert a tensor to single precision representation by using the tensors **.float()** command.\n",
    "\n",
    "For some of the operations, we will also need to convert fixed point tensors (integers) to single precision. This is done in a similar way using the **.float()** command.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a fixed point tensor\n",
    "x_int = torch.tensor([4, 2, 3])\n",
    "print('x_int=\\n{}'.format(x_int))\n",
    "print('x_int.dtype={}\\n'.format(x_int.dtype))\n",
    "\n",
    "## Converting the tensor to single persicion\n",
    "x_single = x_int.float()\n",
    "print('x_single=\\n{}'.format(x_single))\n",
    "print('x_single.dtype={}\\n'.format(x_single.dtype))\n",
    "\n",
    "## Converting the tensor to double persicion\n",
    "x_doubel = x_int.double()\n",
    "print('x_doubel=\\n{}'.format(x_doubel))\n",
    "print('x_doubel.dtype={}\\n'.format(x_doubel.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch and GPUs\n",
    "PyTorch provides a simple way to copy a tensor to the GPU's memory. The GPUs In PyTorch are referred to as [**CUDA**](https://en.wikipedia.org/wiki/CUDA) devices. A tensor can be copied to the GPU's memory by using the tensor's **.cuda** command. All the mathematical operations can then be performed on the copied tensor in the same way as if it was in the regular memory. The result of a calculation which was performed on the GPU will be stored on the GPU's memory as well.\n",
    "\n",
    "A tensor can be copied back from the GPU's memory to the regular memory using the **.cpu** command. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moving x1 to the GPU\n",
    "x1_gpu = x1.cuda()\n",
    "print('x1_gpu=\\n{}\\n'.format(x1_gpu))\n",
    "\n",
    "## Moving x2 to the GPU\n",
    "x2_gpu = torch.randint(low=0, high=10, size=(2, 3)).float().cuda()\n",
    "print('x2_gpu=\\n{}\\n'.format(x1_gpu))\n",
    "\n",
    "## Performing matrix multiplication on the GPU\n",
    "y = torch.matmul(x1_gpu.t(), x2_gpu)\n",
    "print('torch.matmul(x1_gpu.t(), x2_gpu)=\\n{}\\n'.format(y))\n",
    "\n",
    "print('y.cpu()=\\n{}'.format(y.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the **device='cuda:0'** which is attached to the outputs of tensors which are stored on the GPU's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ Calculate the multiplication table (לוח הכפל ($\\left[1,2,\\ldots,10\\right]^T\\left[1,2,\\ldots,10\\right]$)) on the GPU, copy the result back to the CPU and print the result:\n",
    "- PYTorch cannot multiply fixed point tensor on the GPU. Therefore so make sure you convert the tensors to single precision tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "mult_table = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print(mult_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating gradients\n",
    "\n",
    "One of PyTorch's main features is its ability to automatically calculate gradients by using back-propagation.\n",
    "\n",
    "To calculate the gradient of a function, we need to preforme the following steps:\n",
    "\n",
    "1. Select the variables according to which we would want to calculate the derivative.\n",
    "2. Clear all previous gradient calculations.\n",
    "3. Calculate the result of the functions for a given set of variables. (the forward path)\n",
    "4. Run the back-propagation function starting from the calculated result of the function.\n",
    "\n",
    "Let us start with an example, and then explain it.\n",
    "\n",
    "The following code calculates the following derivative: $\\left.\\frac{\\partial}{\\partial x}x^2+5x+4\\right|_{x=3}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the variables which we would want to calculate the derivative according to\n",
    "x = torch.tensor(3).float()\n",
    "x.requires_grad = True\n",
    "\n",
    "## Calculate the function's result\n",
    "y = x ** 2 + 5 * x + 4\n",
    "\n",
    "## Run back-propagation\n",
    "y.backward()\n",
    "\n",
    "## Prin the result\n",
    "x_grad = x.grad\n",
    "print('The derivative is: {}'.format(x_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we have performed the following steps:\n",
    "\n",
    "1. We have first defined a tensor **x**, and then marked be setting it's **.requires_grad** field to **True**. This tells PyTorch that we will later want to calculate the derivative according to it.\n",
    "2. We have calculated the function's result (this is the forward path).\n",
    "3. We have used the result of the function to initiate the back-propagation calculation by using the **.backword()** function of the result tensor.\n",
    "\n",
    "After the back-propagation step, the derivative of the function according to each one of the selected variables will be stored in the **.grad** field of each of the variables.\n",
    "\n",
    "In this case, we did not have to clear any previous calculation since we did yet run any backward calculation using these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ Calculate and plot the derivative of the sigmoid function $\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.arange(-10, 10, 0.1)\n",
    "res = np.zeros_like(vals)\n",
    "for i in range(len(vals)):\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    x = torch.tensor(vals[i]).float()\n",
    "    ...\n",
    "    res[i] = x.grad\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(vals, res);\n",
    "ax.set_title('$\\\\frac{\\\\partial}{\\\\partial x}\\\\frac{1}{1+e^{-x}}$', fontsize=20)\n",
    "ax.set_xlabel('$x$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Repeat the preparation of the data in the same manner as in the last assignment, but this time **do not add the additional constant 1** to the features vector. \n",
    "\n",
    "✍️ Complete the code below to load the data, split it, and extract the PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "def load_lfw_dataset():\n",
    "    \"\"\"\n",
    "    Loading the Labeled faces in the Wild dataset.\n",
    "    Load only face of persons which appear at least 50 times in the dataset.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in the dataset.\n",
    "    - H: the images' height\n",
    "    - W: the images' width\n",
    "    - K: The number of classes.\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "    n_classes: int\n",
    "        The number of different classes, K.\n",
    "    label_to_name_mapping: list\n",
    "        A list of K strings containing the name related to each label.\n",
    "    image_shape: list\n",
    "        The image's shape as the list: [H, W]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = fetch_lfw_people(min_faces_per_person=50)\n",
    "\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return x, y, n_classes, label_to_name_mapping, image_shape\n",
    "\n",
    "x, y, n_classes, label_to_name_mapping, image_shape = load_lfw_dataset()\n",
    "\n",
    "print('Number of images in the dataset: {}'.format(len(x)))\n",
    "print('Number of different persons in the dataset: {}'.format(n_classes))\n",
    "print('Each images size is: {}'.format(image_shape))\n",
    "\n",
    "_, images_per_class = np.unique(y, return_counts=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(label_to_name_mapping, images_per_class)\n",
    "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
    "ax.set_title('Images per person')\n",
    "ax.set_ylabel('Number of images')\n",
    "\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x[i], cmap='gray')\n",
    "    ax.set_ylabel(label_to_name_mapping[y[i]])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, train_fraction=0.6, validation_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Split the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "    train_fraction: float\n",
    "        The fraction of the dataset to use as the train set.\n",
    "    validation_fraction: float\n",
    "        The fraction of the dataset to use as the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_samples_train: int\n",
    "        The number of train samples.\n",
    "    x_train: ndarray\n",
    "        The n_samples_train x H x W array of train images.\n",
    "    y_train: ndarray\n",
    "        The 1D array of length n_samples_train of train labels.\n",
    "    n_samples_val: int\n",
    "        The number of validation samples.\n",
    "    x_val: ndarray\n",
    "        The n_samples_val x H x W array of validation images.\n",
    "    y_val: ndarray\n",
    "        The 1D array of length n_samples_val of validation labels.\n",
    "    n_samples_test: int\n",
    "        The number of test samples.\n",
    "    x_test: ndarray\n",
    "        The n_samples_test x H x W array of test images.\n",
    "    y_test: ndarray\n",
    "        The 1D array of length n_samples_test of test labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ## Create a random generator using a fixed seed\n",
    "    rand_gen = np.random.RandomState(0)\n",
    "\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return n_samples_train, x_train, y_train, n_samples_val, x_val, y_val, n_samples_test, x_test, y_test\n",
    "\n",
    "n_samples_train, x_train, y_train, n_samples_val, x_val, y_val, n_samples_test, x_test, y_test = split_dataset(x, y)\n",
    "\n",
    "print('Number of training samples: {}'.format(n_samples_train))\n",
    "print('Number of validation samples: {}'.format(n_samples_val))\n",
    "print('Number of test samples: {}'.format(n_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def generate_pca_object(x_train, n_pca_components=300):\n",
    "    \"\"\"\n",
    "    Generate a training sklearn.decomposition.PCA object.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in x_train.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train: ndarray\n",
    "        The N x H x W array of train images.\n",
    "    n_pca_components: int\n",
    "        The number of PCA components to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca: sklearn.decomposition.PCA \n",
    "        The trained sklearn.decomposition.PCA object which can perform the PCA decomposition and reconstruction\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return pca\n",
    "\n",
    "\n",
    "def extract_features(x, pca):\n",
    "    \"\"\"\n",
    "    Extract features from the images, which include the PCA elements and the constant 1.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in x.\n",
    "    - D_PCA: The number of components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images (x can be either the train, validation, or test dataset).\n",
    "    pca: sklearn.decomposition.PCA \n",
    "        The trained sklearn.decomposition.PCA object which can perform the PCA decomposition and reconstruction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features: ndarray\n",
    "        The N x (D_PCA + 1) array of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "   \n",
    "    return features\n",
    "\n",
    "pca = generate_pca_object(x_train)\n",
    "features_train = extract_features(x_train, pca)\n",
    "features_val = extract_features(x_val, pca)\n",
    "features_test = extract_features(x_test, pca)\n",
    "\n",
    "## Ploting the reconstruction of the first 10 test images\n",
    "reconstructed_images_flat = pca.inverse_transform(features_test)\n",
    "reconstructed_images = reconstructed_images_flat.reshape(-1, *image_shape)\n",
    "fig, ax_array = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    ax_array[0][i].imshow(x_test[i], cmap='gray')\n",
    "    ax_array[0][i].set_yticks([])\n",
    "    ax_array[0][i].set_xticks([])\n",
    "\n",
    "    ax_array[1][i].imshow(reconstructed_images[i], cmap='gray')\n",
    "    ax_array[1][i].set_yticks([])\n",
    "    ax_array[1][i].set_xticks([])\n",
    "ax_array[0][0].set_ylabel('Original')\n",
    "ax_array[1][0].set_ylabel('Reconstructed')\n",
    "fig.suptitle('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert all the data to tensors, transfer it to the GPU and to single precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gpu = torch.tensor(x_train).float().cuda()\n",
    "features_train_gpu = torch.tensor(features_train).float().cuda()\n",
    "y_train_gpu = torch.tensor(y_train).cuda()\n",
    "\n",
    "x_val_gpu = torch.tensor(x_val).float().cuda()\n",
    "features_val_gpu = torch.tensor(features_val).float().cuda()\n",
    "y_val_gpu = torch.tensor(y_val).cuda()\n",
    "\n",
    "x_test_gpu = torch.tensor(x_test).float().cuda()\n",
    "features_test_gpu = torch.tensor(features_test).float().cuda()\n",
    "y_test_gpu = torch.tensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic Regression\n",
    "\n",
    "We will start by returning to the logistic regression model from last assignment.\n",
    "\n",
    "**Reminder**: We are modeling the conditional distribution of the labels as:\n",
    "$$\n",
    "p\\left(y|\\boldsymbol{x};\\Theta\\right)=\\sigma\\left(\\Theta\\boldsymbol{x},y\\right)\n",
    "$$\n",
    "\n",
    "And our objective is to minimize the log likelihood of this probability:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{\\arg\\min}-\\frac{1}{N}\\sum_i\\log\\left(\\sigma\\left(\\Theta\\boldsymbol{x}_i,y_i\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where $\\sigma$ is the softmax function:\n",
    "\n",
    "$$\n",
    "\\sigma\\left(\\boldsymbol{q},k\\right)=\\frac{e^{q_k}}{\\sum_{k'} e^{q_{k'}}}\n",
    "$$\n",
    "\n",
    "✍️ Complete the code below to define the same objective function we defined last time:\n",
    "$$\n",
    "g\\left(\\Theta;X,\\boldsymbol{y}\\right)=-\\frac{1}{N}\\sum_i\\log\\left(\\sigma\\left(\\Theta\\boldsymbol{x}_i,y_i\\right)\\right)\n",
    "$$\n",
    "This time, implement the function by using the two following function from torch:\n",
    "\n",
    "- [**torch.nn.functional.log_softmax**](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax): which calculates the log of the softmax function. This function is similar to the softmax function which you have implemented in the last assignment with the addition of taking the log of the result.\n",
    "- [**torch.nn.functional.nll_loss**](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.nll_loss): Which calculates the negative log-likelihood of a matrix of log-probabilities, $P$, and a vector of labels, $\\boldsymbol{y}$: $-\\frac{1}{N}\\sum_iP_{i,y_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the objective function\n",
    "def g(theta, x, y):\n",
    "    \"\"\"\n",
    "    The objective function.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The number of classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix.\n",
    "    x: ndarray\n",
    "        The N x D features matrix.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res: float\n",
    "        The objective function evaluated at the given theta.\n",
    "    \"\"\"\n",
    "\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "    return res\n",
    "\n",
    "## Testing the function\n",
    "test_theta = torch.tensor([[1, -3, 2], [-2, 1, -1]]).float().cuda()\n",
    "test_x = torch.tensor([[0.1, 0.7, -0.2], [0.5, -0.2, 0.5]]).float().cuda()\n",
    "test_y = torch.tensor([1, 0]).cuda()\n",
    "print(float(g(test_theta, test_x, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the result you get is: $0.033094\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it the PyTorch way - Using Torch.nn.Sequential and  a Loss Function\n",
    "\n",
    "An alternative way to define the model function, is by using the **[torch.nn.Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)** operator. The Senquential operator allows us to define a model by stacking together a chain of operators. This method is usefull when stacking layers of a neural-network model, as we will see later.\n",
    "\n",
    "In addition to the model, we need to also define our loss function between the output of the model and the labels. PyTorch offers a large variety of [such loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "In the case of logistic regression, the model is simply a linear (fully connected) layer followed by a softmax operation. The loss function in our case is the minus log-likelihood function. Therefore, we can calculate the model in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=3, out_features=2, bias=False),\n",
    "    torch.nn.LogSoftmax(dim=1),\n",
    "    ).cuda()\n",
    "\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "\n",
    "## Testing the function\n",
    "model[0].weight[:] = torch.tensor([[1, -3, 2], [-2, 1, -1]]).float().cuda()\n",
    "test_x = torch.tensor([[0.1, 0.7, -0.2], [0.5, -0.2, 0.5]]).float().cuda()\n",
    "test_y = torch.tensor([1, 0]).cuda()\n",
    "\n",
    "log_prob = model(test_x)\n",
    "loss = loss_func(log_prob, test_y)\n",
    "\n",
    "print(float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we used have used the torch.nn.Sequential to stack together a [**torch.nn.Linear**](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) layer and a [**torch.nn.LogSoftmax**](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax) layer. \n",
    "\n",
    "The linear layer is defined by the number of **input_features**, the number of **output_features** and a flag for optionally added a bias term.\n",
    "\n",
    "The log-softmax layer is defined by the dimension in which the softmax is calculated.\n",
    "\n",
    "Notice that we have used here  [**torch.nn.LogSoftmax**](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax) to define an operator (rather then  [**torch.nn.functional.log_softmax**](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax) which is a function)\n",
    "\n",
    "In addition, notice that we have used the **.cuda** function to copy all the model parameters to the GPU.\n",
    "\n",
    "The parameters of each operator are automatically defined for each operator (which in this case, is the matrix $\\Theta$ of the linear layer). They are stored as part of the model inside each of the operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Decent Algorithm\n",
    "\n",
    "We will use the following function for running the gradient descent algorithm with an L2 regularization.\n",
    "\n",
    "A few points which are worth mentioning regarding the code:\n",
    "1. It uses the built-in optimization object [**torch.optim.SGD**](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) for performing the gradient step.\n",
    "2. We are using the weight decay option of torch.optim.SGD which is equivalet to adding an L2 regularization to the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, alpha, n_iters, x_train, y_train, x_val, y_val, llambda):\n",
    "    ## Initialize lists to store intermidiate results for plotting\n",
    "    objective_list_train = []\n",
    "    objective_list_val = []\n",
    "    \n",
    "    ## Defining the loss function\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    \n",
    "    ## Defein Optimizer\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=alpha, weight_decay=2 * llambda)\n",
    "    \n",
    "    ## Perforing the update steps\n",
    "    for i_iter in tqdm.tqdm_notebook(range(n_iters)):\n",
    "        ## reseting all previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Forward path\n",
    "        log_prob = model(x_train)\n",
    "        log_prob = log_prob.view(log_prob.shape[0], log_prob.shape[1]) ## This is for later support for the CNNs\n",
    "        loss = loss_func(log_prob, y_train)\n",
    "        \n",
    "        ## Backward path\n",
    "        loss.backward()\n",
    "        \n",
    "        ## Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Store intermidiate results\n",
    "        objective_list_train.append(float(loss))\n",
    "        with torch.no_grad(): ## This line is important and it tells PyTorch nit to calculate gradiants in this section \n",
    "            log_prob = model(x_val)\n",
    "            log_prob = log_prob.view(log_prob.shape[0], log_prob.shape[1])  ## This is for later support for the CNNs\n",
    "            loss = loss_func(log_prob, y_val)\n",
    "            objective_list_val.append(float(loss))\n",
    "    \n",
    "    objectives_array_train = np.array(objective_list_train)\n",
    "    objectives_array_val = np.array(objective_list_val)\n",
    "    \n",
    "    return objectives_array_train, objectives_array_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "✍️ Complete the code below to define the logistig regression model and used the above function to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = features_train_gpu.shape[1]\n",
    "\n",
    "## The logistic regression model\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "model = torch.nn.Sequential(\n",
    "    ...\n",
    "    ).cuda()\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## To save you time, some optimal hyper-parameters were pre-selected. \n",
    "alpha = 1e-5  ## Learning rate parameter\n",
    "llambda = 100  ## L2 regularization parameter\n",
    "n_iters = 2000\n",
    "objectives_array_train, objectives_array_val = train_model(model, alpha, n_iters, \n",
    "                                                           features_train_gpu, y_train_gpu, \n",
    "                                                           features_val_gpu, y_val_gpu,\n",
    "                                                           llambda)\n",
    "\n",
    "## Plot the objective\n",
    "## ==================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('Otimization objective')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.set_ylim(0, 5)\n",
    "ax.legend();\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat_test = model(features_test_gpu).argmax(dim=1)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test_gpu).float().mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))\n",
    "\n",
    "## Plot estimation\n",
    "## ===============\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x_test[i], cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[y_hat_test[i]].split()[-1],\n",
    "                   color='black' if y_hat_test[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Now that we have a training function and we know how to define models using PyTorch, we can start playing around with some neural-networks architectures. \n",
    "\n",
    "Specifically, we will run one MLP network and one CNN network.\n",
    "\n",
    "✍️ Complete the code below to define an MLP with 1 hidden layer of 1024 neurons and a ReLU activation function.\n",
    "\n",
    "I.e., build a network which with of the following layers:\n",
    "\n",
    "1. A fully connected (linear) layer with an input of the n_features and output of 1024.\n",
    "2. A ReLU layer\n",
    "3. A fully connected (linear) layer with an input of 1024 and output of n_classes.\n",
    "4. A log-softmax function.\n",
    "\n",
    "- Use [**torch.nn.ReLU**(https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU)] to define the ReLU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "model = torch.nn.Sequential(\n",
    "    ...\n",
    "    ).cuda()\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## We will use the following \n",
    "alpha = 1e-3\n",
    "llambda = 0.1\n",
    "n_iters = 20000\n",
    "objectives_array_train, objectives_array_val = train_model(model, alpha, n_iters, \n",
    "                                                           features_train_gpu, y_train_gpu, \n",
    "                                                           features_val_gpu, y_val_gpu,\n",
    "                                                           llambda)\n",
    "\n",
    "\n",
    "## Plot the objective\n",
    "## ==================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('Otimization objective')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.set_ylim(0, 5)\n",
    "ax.legend();\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat_test = model(features_test_gpu).argmax(dim=1)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test_gpu).float().mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you get a test risk of about 16%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "As opposed to using the PCA features as an input to our mode,l we can use the raw images directly, but for that, we will need a different architecture.\n",
    "\n",
    "✍️ Complete the code below to define a CNN which is composed of the following layers:\n",
    "\n",
    "1. A convolutional layer with a 4x4 kernel, 64 output channels, a stride of 2, and a padding of 2 on the vertical direction and 4 on the horizontal direction.\n",
    "2. A ReLU layer.\n",
    "3. A convolutional layer with a 4x4 kernel, 128 output channels, a stride of 2, and a padding of 1 in each direction.\n",
    "4. A ReLU layer.\n",
    "5. A convolutional layer with a 4x4 kernel, 256 output channels, a stride of 2, and a padding of 1 in each direction.\n",
    "6. A ReLU layer.\n",
    "7. A convolutional layer with a 4x4 kernel, 512 output channels, a stride of 2, and a padding of 1 in each direction.\n",
    "8. A ReLU layer.\n",
    "9. A convolutional layer (which is also a fully connected layer) with a 4x3 kernel and n_classes output channels (with no padding)\n",
    "10. A log-softmax layer.\n",
    "\n",
    "\n",
    "- Use [**torch.nn.MaxPool2d**(https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)] to define the max pooling layers. Oprn the documentation to check the function parameters.\n",
    "- Implement the fully connected layer using a convolutional layer with a kernel of size 1,\n",
    "- The learning rate, which was pre-selected for training the network, is a bit too high for the task and was chosen to produce reasonable results in reasonable time. Training the network with the given number of iteration should take about 5 minutes. \n",
    "- This architecture is very far from being optimal and was built to be simple, and with a reasonable training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "model = torch.nn.Sequential(\n",
    "    ...\n",
    "    ).cuda()\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "alpha = 1e-3\n",
    "llambda = 0.1\n",
    "n_iters = 1000\n",
    "objectives_array_train, objectives_array_val = train_model(model, alpha, n_iters, \n",
    "                                                           x_train_gpu[:, None, :, :], y_train_gpu, \n",
    "                                                           x_val_gpu[:, None, :, :], y_val_gpu,\n",
    "                                                           llambda)\n",
    "\n",
    "\n",
    "## Plot the objective\n",
    "## ==================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('Otimization objective')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.set_ylim(0, 5)\n",
    "ax.legend();\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat_test = model(x_test_gpu[:, None, :, :]).argmax(dim=1)[:, 0, 0]\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test_gpu).float().mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: Submission\n",
    "\n",
    "To submit your code, download it as a **ipynb** file from Colab, and upload it to the course's website. You can download this code by selecting **Download .ipynb** from the **file** menu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
