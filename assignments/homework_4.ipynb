{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework assignment 4 - Logistic Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will return to the face recognition and solve it using linear logistic regression. We will use a simple gradient descent algorithm for solving the MLE optimization problem for the linear logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminders\n",
    "\n",
    "- Start by making a copy of this notebook in order to be able to save it.\n",
    "- Use **Ctrl+[** to expend all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip of the day - Debugging with PDB (optional, for the advanced users):\n",
    "\n",
    "One of the main problems in working with Colab / Jupyter is the lack of a good debugger.\n",
    "\n",
    "A not so optimal solution (but sometimes better than nothing) is to use Python's built-in debugger. Python comes with a very basic command line based debugger called [PDB](https://docs.python.org/3/library/pdb.html). It has all the basic debugger capabilities but is missing a good interface.\n",
    "\n",
    "You can drop into debug mode in the middle of any Python code simply by placing the command **pdb.set_trace()** before the line which you want to debug (only after importing the **pdb** package). Once in debug mode, the debugger prompt will appear in which you can use the following commands:\n",
    "\n",
    "- **l** or **list**: to print the current line and the surrounding code.\n",
    "- **n** or **next**: to run the current line.\n",
    "- **c** or **continue**: to continue running until a breakpoint or until the end of the function.\n",
    "- **q** or **quit**: to stop the run and exit the debugger.\n",
    "- **b \\<number\\>**: Place a breakpoint in line \\<number\\>\n",
    "- **!\\<python expression\\>**: to run any python expression.\n",
    "\n",
    "(This is only a partial list of the PDB's commands. For the full list of commands you can refer to the [official documentation](https://docs.python.org/2/library/pdb.html#debugger-commands), or at [this cheat sheet](https://kapeli.com/cheat_sheets/Python_Debugger.docset/Contents/Resources/Documents/index))\n",
    "\n",
    "Let us look at an example:\n",
    "\n",
    "- Add the line **pdb.set_trace()** to the following code just before the **x **= 2**  line, and execute the cell to drop into debug mode.\n",
    "- Type **l** (followed by **Enter**) to print the current line and surrounding code.\n",
    "- Type **!print(x)** to print the value of the variable **x** (you can also omit the print command and just use **!x** in this case).\n",
    "- Type **!x=2** to change the value of **x**.\n",
    "- Type **n** execute the next line.\n",
    "- Type **l** again.\n",
    "- Type **!print(x)** again.\n",
    "- Type **b 10** to place a breakpoint on the **return x** line.\n",
    "- Type **c** to run the code until the breakpoint.\n",
    "- Type **l** again.\n",
    "- Type **!print(x)** again.\n",
    "- Type **c** or **q** to quit the debugger with or without finishing to run the code\n",
    "- After you finish playing with the debugger make sure you remove or comment out the **pdb.set_trace()** line.\n",
    "- Clear our all the debugger output (before submitting the code). You can do so, for example, by pressing the **X** button left to the cell's output area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def func(x):\n",
    "    x += 2\n",
    "    x *= 4\n",
    "    x -= 2\n",
    "    x /= 2\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(func(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! Important\n",
    "\n",
    "- The **pdb.set_trace()** command can be placed anywhere, but stepping through the code (using the **n** command) is only possible inside functions.\n",
    "- **You must exit the debugger** (using **c** or **q**) in order to be able to run any other cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your IDs\n",
    "\n",
    "✍️ Fill in your IDs in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "# Replace the IDs bellow with our own\n",
    "student1_id = '012345678'\n",
    "student2_id = '012345678'\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('Hello ' + student1_id + ' & ' + student2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "Importing the NumPy, Pandas and Matplotlib packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## This line makes matplotlib plot the figures inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "## Set some default values of the the matplotlib plots\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
    "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "As stated earlier, we will work again with the same dataset as in the last assignment (the Labeled Faces in the Wild dataset), and we will also use the same PCA features. We will make 2 significant changes in the preparation process from last time:\n",
    "1. We will split the data into 60% train / 20 % validation / 20% test.\n",
    "2. We will add an additional constant value of 1 to the feature vector. In other words, change the feature vector from $\\left[\\phi_1(x), \\phi_2(x), \\ldots, \\phi_n(x)\\right]^T$ to $\\left[\\phi_1(x), \\phi_2(x), \\ldots, \\phi_n(x), 1\\right]^T$.\n",
    "\n",
    "✍️ Complete the code below to load the data, split it, and extract the PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "def load_lfw_dataset():\n",
    "    \"\"\"\n",
    "    Loading the Labeled faces in the Wild dataset.\n",
    "    Load only face of person which appear at least 50 times in the dataset.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in the dataset.\n",
    "    - H: the images' height\n",
    "    - W: the images' width\n",
    "    - K: The number of classes.\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "    n_classes: int\n",
    "        The number of different classes, K.\n",
    "    label_to_name_mapping: list\n",
    "        A list of K strings containing the name related to each label.\n",
    "    image_shape: list\n",
    "        The image's shape as the list: [H, W]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = fetch_lfw_people(min_faces_per_person=50)\n",
    "\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    n_classes = y.max() + 1  ## The number of classes.\n",
    "    \n",
    "    return x, y, n_classes, label_to_name_mapping, image_shape\n",
    "\n",
    "x, y, n_classes, label_to_name_mapping, image_shape = load_lfw_dataset()\n",
    "\n",
    "print('Number of images in the dataset: {}'.format(len(x)))\n",
    "print('Number of different persons in the dataset: {}'.format(n_classes))\n",
    "print('Each images size is: {}'.format(image_shape))\n",
    "\n",
    "_, images_per_class = np.unique(y, return_counts=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(label_to_name_mapping, images_per_class)\n",
    "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
    "ax.set_title('Images per person')\n",
    "ax.set_ylabel('Number of images')\n",
    "\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x[i], cmap='gray')\n",
    "    ax.set_ylabel(label_to_name_mapping[y[i]])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x, y, train_fraction=0.6, validation_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Split the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "    train_fraction: float\n",
    "        The fraction of the dataset to use as the train set.\n",
    "    validation_fraction: float\n",
    "        The fraction of the dataset to use as the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_samples_train: int\n",
    "        The number of train samples.\n",
    "    x_train: ndarray\n",
    "        The n_samples_train x H x W array of train images.\n",
    "    y_train: ndarray\n",
    "        The 1D array of length n_samples_train of train labels.\n",
    "    n_samples_val: int\n",
    "        The number of validation samples.\n",
    "    x_val: ndarray\n",
    "        The n_samples_val x H x W array of validation images.\n",
    "    y_val: ndarray\n",
    "        The 1D array of length n_samples_val of validation labels.\n",
    "    n_samples_test: int\n",
    "        The number of test samples.\n",
    "    x_test: ndarray\n",
    "        The n_samples_test x H x W array of test images.\n",
    "    y_test: ndarray\n",
    "        The 1D array of length n_samples_test of test labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return n_samples_train, x_train, y_train, n_samples_val, x_val, y_val, n_samples_test, x_test, y_test\n",
    "\n",
    "n_samples_train, x_train, y_train, n_samples_val, x_val, y_val, n_samples_test, x_test, y_test = split_dataset(x, y)\n",
    "\n",
    "print('Number of training samples: {}'.format(n_samples_train))\n",
    "print('Number of validation samples: {}'.format(n_samples_val))\n",
    "print('Number of test samples: {}'.format(n_samples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def generate_pca_object(x_train, n_pca_components=300):\n",
    "    \"\"\"\n",
    "    Generate a training sklearn.decomposition.PCA object.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in x_train.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train: ndarray\n",
    "        The N x H x W array of train images.\n",
    "    n_pca_components: int\n",
    "        The number of PCA components to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca: sklearn.decomposition.PCA \n",
    "        The trained sklearn.decomposition.PCA object which can perform the PCA decomposition and reconstruction\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return pca\n",
    "\n",
    "\n",
    "def extract_features(x, pca):\n",
    "    \"\"\"\n",
    "    Extract features from the images, which include the PCA elements and the constant 1.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples in x.\n",
    "    - D_PCA: The number of components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        The N x H x W array of images (x can be either the train, validation or test dataset).\n",
    "    pca: sklearn.decomposition.PCA \n",
    "        The trained sklearn.decomposition.PCA object which can perform the PCA decomposition and reconstruction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features: ndarray\n",
    "        The N x (D_PCA + 1) array of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "     ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "   \n",
    "    return features\n",
    "\n",
    "pca = generate_pca_object(x_train)\n",
    "features_train = extract_features(x_train, pca)\n",
    "features_val = extract_features(x_val, pca)\n",
    "features_test = extract_features(x_test, pca)\n",
    "\n",
    "## Ploting the reconstruction of the first 10 test images\n",
    "reconstructed_images_flat = pca.inverse_transform(features_test[:, :-1])\n",
    "reconstructed_images = reconstructed_images_flat.reshape(-1, *image_shape)\n",
    "fig, ax_array = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    ax_array[0][i].imshow(x_test[i], cmap='gray')\n",
    "    ax_array[0][i].set_yticks([])\n",
    "    ax_array[0][i].set_xticks([])\n",
    "\n",
    "    ax_array[1][i].imshow(reconstructed_images[i], cmap='gray')\n",
    "    ax_array[1][i].set_yticks([])\n",
    "    ax_array[1][i].set_xticks([])\n",
    "ax_array[0][0].set_ylabel('Original')\n",
    "ax_array[1][0].set_ylabel('Reconstructed')\n",
    "fig.suptitle('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "**Notation**: For simplicity, from here on we will use $\\boldsymbol{x}$ to donate the feature vector (instead of using $\\boldsymbol{\\phi\\left(\\boldsymbol{x}\\right)}$). in the code make sure to use the features vectors/matrices in all calculations.\n",
    "\n",
    "The multi-class linear logistic regression model models the conditional probability of the labels using the following model:\n",
    "$$\n",
    "p\\left(y|\\boldsymbol{x};\\left\\{\\boldsymbol{\\theta}_{k}\\right\\}\\right)=\\frac{e^{\\boldsymbol{\\theta}_{y}^T\\boldsymbol{x}}}{\\sum_{k'} e^{\\boldsymbol{\\theta}_{k'}^T\\boldsymbol{x}}}\n",
    "$$\n",
    "\n",
    "Note that in the multi-class case, we have multiple $\\boldsymbol{\\theta}_k$ vectors (one for each class).\n",
    "\n",
    "**A comment**: We saw in class that it is common to set $\\boldsymbol{\\theta}_0=\\vec{0}$ without loss of generality, we will not do so where in order to keep our code simple.\n",
    "\n",
    "We shall define the following notations:\n",
    "- $D$: the number of features.\n",
    "- $N$: the number of samples.\n",
    "- $K$: the number of classes.\n",
    "- $X$: The matrix of features:\n",
    "$$\n",
    "X=\n",
    "\\left[\\begin{matrix}\n",
    "\\boldsymbol{x}_0^T \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{x}_{N-1}^T \\\\\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\left[\\begin{matrix}\n",
    "x_{0,0} & x_{0,1} & \\ldots & x_{0,D-1} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "x_{N-1,0} & x_{N-1,1} & \\ldots & x_{N-1,D-1} \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "- $\\Theta$: The matrix of all parameters of the model:\n",
    "$$\n",
    "\\Theta=\n",
    "\\left[\\begin{matrix}\n",
    "\\boldsymbol{\\theta}_0^T \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{\\theta}_{K-1}^T \\\\\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\left[\\begin{matrix}\n",
    "\\theta_{0,0} & \\theta_{0,1} & \\ldots & \\theta_{0,D-1} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\theta_{K-1,0} & \\theta_{K-1,1} & \\ldots & \\theta_{K-1,D-1} \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "- $\\sigma\\left(\\boldsymbol{q}\\right)$:The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) which is define as:\n",
    "$$\n",
    "\\sigma\\left(\\boldsymbol{q},k\\right)=\\frac{e^{q_k}}{\\sum_{k'} e^{q_{k'}}}\n",
    "$$\n",
    "\n",
    "Using the softmax function, we can write the conditional probability of the labels as:\n",
    "$$\n",
    "p\\left(y|\\boldsymbol{x};Theta\\right)=\\sigma\\left(\\Theta\\boldsymbol{x},y\\right)\\left(=\\frac{e^{\\boldsymbol{\\theta}_{y}^T\\boldsymbol{x}}}{\\sum_{k'} e^{\\boldsymbol{\\theta}_{k'}^T\\boldsymbol{x}}}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ Complete the code below to define the functions calculating the softmax function:\n",
    "\n",
    "- The code should in fact calculate the softmax function for $N$ vectors $\\boldsymbol{q}_i$ and for all $k$ values, i.e. it perform the following calculation:\n",
    "$$\n",
    "f\\left(\\left[\\begin{matrix}\n",
    "\\boldsymbol{q}_0^T \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{q}_{N-1}^T \\\\\n",
    "\\end{matrix}\\right]\\right)\n",
    "=\n",
    "\\left[\\begin{matrix}\n",
    "\\sigma\\left(\\boldsymbol{q}_0,0\\right) & \\sigma\\left(\\boldsymbol{q}_0,1\\right) & \\ldots & \\sigma\\left(\\boldsymbol{q}_0,K-1\\right) \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\sigma\\left(\\boldsymbol{q}_{N-1},0\\right) & \\sigma\\left(\\boldsymbol{q}_{N-1},1\\right) & \\ldots & \\sigma\\left(\\boldsymbol{q}_{N-1},K-1\\right) \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the softmax function\n",
    "def softmax(q_mat):\n",
    "    \"\"\"\n",
    "    A function which calculates the softmax function for a matrix which contains N q-vectors.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of q vectors.\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_mat: ndarray\n",
    "        The N x K array of the N q vectors: q{1},...,q{N-1}\n",
    "        \n",
    "        q_mat = [[q{0}{1}, q{0}{2}, ..., q{0}{K-1}], \n",
    "                 ...,\n",
    "                 [q{N-1}{1}, q{N-1}{2}, ..., q{N-1}{K-1}]]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res: ndarray\n",
    "        The N x K array of the output of the soft max function for every n and every k:\n",
    "        res = [[sigma(q{0},0), sigma(q{0},1), ..., sigma(q{0},K-1)], \n",
    "               ...,\n",
    "               [sigma(q{N-1},0), sigma(q{N-1},1), ..., sigma(q{N-1},K-1)]]\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return res\n",
    "\n",
    "## Testing the function\n",
    "test_q_mat = np.array([[1, -3, 2],\n",
    "                       [-2, 1, -1]])\n",
    "print(softmax(test_q_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the result you get is:\n",
    "```\n",
    "[[0.26762315 0.00490169 0.72747516]\n",
    " [0.04201007 0.84379473 0.1141952 ]]\n",
    "```\n",
    "(notice that each row must be a valid distribution matrix, i.e. positive and sum up to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The MLE problem\n",
    "\n",
    "To select the models parameters $\\Theta$ we would use MLE. The MLE problem is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\theta}^*\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\max}\\ \\mathcal{L}\\left(\\left\\{\\boldsymbol{\\theta}_k\\right\\};\\left\\{\\boldsymbol{x}_i,y_i\\right\\}\\right) \\\\\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\min}\\ -l\\left(\\left\\{\\boldsymbol{\\theta}_k\\right\\};\\left\\{y_i,\\boldsymbol{x}_i\\right\\}\\right) \\\\\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\min}-\\frac{1}{N}\\sum_i\\log\\left(p\\left(y_i|\\boldsymbol{x}_i;\\left\\{\\boldsymbol{\\theta}_k\\right\\}\\right)\\right) \\\\\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\min}-\\frac{1}{N}\\sum_i\\log\\left(\\sigma\\left(\\Theta\\boldsymbol{x},y_i\\right)\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(For convenience we have add a $\\frac{1}{N}$ term before the summation. This does not effect the selection of the optimal $\\Theta$, but prevents the objective to scale with the number of samples) \n",
    "\n",
    "We will solve this optimization problem using the gradient descent algorithm. The objective function $g$ of the optimization problem is:\n",
    "\n",
    "$$\n",
    "g\\left(\\Theta;X,\\boldsymbol{y}\\right)=-\\frac{1}{N}\\sum_i\\log\\left(\\sigma\\left(\\Theta\\boldsymbol{x}_i,y_i\\right)\\right)\n",
    "$$\n",
    "\n",
    "✍️ Complete the code bellow to define the objective function $g$:\n",
    "\n",
    "- Use the **softmax** function which we have defined above.\n",
    "- If needed, use a loop over the $K$ classes along with and a vector of indices: **indices = (y == k)**. Do not use a loop over the $N$ samples.\n",
    "- You can also implement the function without any loops at all (which is slightly more efficient and elegant). This can be does using the following [integer array indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing): **arr\\[np.range(len(y)),y\\]** which selects the following sub array: **\\[arr[0,y_0],arr[1,y_1],...,arr[N,y_N]\\]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the objective function\n",
    "def g(theta, x, y):\n",
    "    \"\"\"\n",
    "    The objective function.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of q vectors.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix.\n",
    "    x: ndarray\n",
    "        The N x D features matrix.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res: float\n",
    "        The objective function evaluated at the given theta.\n",
    "    \"\"\"\n",
    "\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return res\n",
    "\n",
    "## Testing the function\n",
    "test_theta = np.array([[1, -3, 2],\n",
    "                       [-2, 1, -1]])\n",
    "test_x = np.array([[0.1, 0.7, -0.2],\n",
    "                   [0.5, -0.2, 0.5]])\n",
    "test_y = np.array([1, 0])\n",
    "print(g(test_theta, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the result you get is: $0.033094\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Gradient\n",
    "\n",
    "For the gradient decent algorithm we will need to be able to calculate $\\nabla_\\Theta g\\left(\\Theta;X,\\boldsymbol{y}\\right)$. \n",
    "\n",
    "Note that $\\Theta$ here is a matrix and not a vector. The notation $\\nabla_\\Theta g\\left(\\Theta;X,\\boldsymbol{y}\\right)$ means:\n",
    "$$\n",
    "\\nabla_\\Theta g\\left(\\Theta\\right)=\n",
    "\\left[\\begin{matrix}\n",
    "\\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{0,0}} & \\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{0,1}} & \\ldots & \\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{0,D-1}} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{K-1,0}} & \\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{K-1,1}} & \\ldots & \\frac{\\partial g\\left(\\Theta\\right)}{\\partial\\theta_{K-1,D-1}} \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "Calculate $\\nabla_\\Theta g\\left(\\Theta;X,\\boldsymbol{y}\\right)$ using the following steps:\n",
    "\n",
    "1. $\\frac{\\partial}{\\partial q_\\tilde{k}}\\sigma\\left(\\boldsymbol{q},k\\right)=\\sigma\\left(\\boldsymbol{q},k\\right)\\left(\\delta_{k,\\tilde{k}}-\\sigma\\left(\\boldsymbol{q},k\\right)\\right)$.\n",
    "\n",
    "(Here $\\delta_{k,\\tilde{k}}$ is the Kronecker delta:  $\\delta_{k,\\tilde{k}}=I\\left\\{k=\\tilde{k}\\right\\}$)\n",
    "\n",
    "2. $-\\frac{\\partial}{\\partial q_\\tilde{k}}\\log\\left(\\sigma\\left(\\boldsymbol{q},k\\right)\\right)=?$\n",
    "\n",
    "3. $-\\frac{\\partial}{\\partial \\theta_{\\tilde{k},j}}\\log\\left(\\sigma\\left(\\Theta\\boldsymbol{x},k\\right)\\right)=\\left.\\left(-\\frac{\\partial}{\\partial q_\\tilde{k}}\\log\\left(\\sigma\\left(\\boldsymbol{q},k\\right)\\right)\\right)\\right|_{\\boldsymbol{q}=\\Theta\\boldsymbol{x}}\\cdot \\frac{\\partial\\boldsymbol{\\theta}_\\tilde{k}^T\\boldsymbol{x}}{\\partial \\theta_{\\tilde{k},j}}=?$\n",
    "\n",
    "4. $\\frac{\\partial}{\\partial \\theta_{k,j}}g\\left(\\Theta;X,\\boldsymbol{y}\\right)=?$\n",
    "\n",
    "Write the last tern as a matrix multiplication of the form:\n",
    "$$\n",
    "\\nabla_\\Theta g\\left(\\Theta\\right)=\\frac{1}{N}AX\n",
    "$$\n",
    "\n",
    "What are the elements $a_{i,j}$ of the matrix $A$? $X$ is the features matrix. Use this matrix $A$ when implementing the gradient function.\n",
    "\n",
    "✍️ Complete the code below to define the functions for calculating $\\nabla_\\Theta g$:\n",
    "- use the **softmax** function .\n",
    "- If needed, use a loop over the $K$ classes, or no loops at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the gradient of h\n",
    "def grad_g(theta, x, y):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of q vectors.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix.\n",
    "    x: ndarray\n",
    "        The N x D features matrix.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res: ndarray\n",
    "        The K x D matrix of gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return res\n",
    "\n",
    "## Compering the function to numeric calculation on random points:\n",
    "n_features = features_train.shape[1]\n",
    "rand_gen = np.random.RandomState(0)\n",
    "eps = 1e-10\n",
    "for i in range(5):\n",
    "    theta = rand_gen.randn(n_classes, n_features) / 100  ## Generate a random theta\n",
    "    k = rand_gen.randint(n_classes) ## Select a random component in of theta\n",
    "    j = rand_gen.randint(n_features)\n",
    "    delta = np.zeros_like(theta) ## Create a small delta_theta vector of the nemerical gradient calculation\n",
    "    delta[k, j] = eps\n",
    "    \n",
    "    numerical_diff = (g(theta + delta, features_train, y_train) - g(theta, features_train, y_train))/ eps\n",
    "    analitical_diff = grad_g(theta, features_train, y_train)[k, j]\n",
    "\n",
    "    print('Test #{}'.format(i))\n",
    "    print('The numerical gradient is: {}'.format(numerical_diff))\n",
    "    print('The analitical gradient is: {}'.format(analitical_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the difference between the analitical and numerical result is smaller then 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Decent Algorithm\n",
    "\n",
    "✍️ Complete the code below to implement a basic gradient descent algorithm (without a stop criteria):\n",
    "\n",
    "- Initialize $\\Theta$ to zero. (This is usually not a good initialization, but we will not discuss this here).\n",
    "- Run **n_iters** steps of the update step: $\\Theta^{\\left(k+1\\right)}=\\Theta^{\\left(k\\right)}-\\alpha\\nabla_\\Theta g\\left(\\Theta^{\\left(k\\right)}\\right)$\n",
    "- For each iteration evaluate and store the value of the objective function using the train set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_gradient_decent(g, grad_g, alpha, n_iters, x_train, y_train, x_val, y_val):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h: funcion\n",
    "        The objective function.\n",
    "    grad_h: funcion\n",
    "        The gradient of the objective function.\n",
    "    alpha: float\n",
    "        The learning rate.\n",
    "    n_iters:\n",
    "        The number of steps to take.\n",
    "    x_train: ndarray\n",
    "        The N x D features matrix of the train set.\n",
    "    y_train: ndarray\n",
    "        The 1D array of length N of labels of the validation set.\n",
    "    x_val: ndarray\n",
    "        The N x D features matrix of the train set.\n",
    "    y_val: ndarray\n",
    "        The 1D array of length N of labels of the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix which the algorithm produces.\n",
    "    objectives_array_train: ndarray\n",
    "        A 1D array of length n_iters which contains the value of the objective \n",
    "        at each step evaluated on the train set.\n",
    "    objectives_array_train: ndarray\n",
    "        A 1D array of length n_iters which contains the value of the objective \n",
    "        at each step evaluated on the validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_classes = max(y_train) + 1\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return theta, objectives_array_train, objectives_array_val\n",
    "\n",
    "alpha = 1e-7\n",
    "n_iters = 500\n",
    "theta, objectives_array_train, objectives_array_val = basic_gradient_decent(g, grad_g, alpha, n_iters, features_train, y_train, features_val, y_val)\n",
    "\n",
    "## Plot the objective\n",
    "## ==================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('Otimization objective')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Learning Rates\n",
    "\n",
    "✍️ Complete the following code to plot the learning curve for the following learning rates: $\\left[10^{-2},10^{-3},10^{-4},10^{-5},10^{-6},10^{-7}\\right]$\n",
    "\n",
    "- It ok if you get some warnings. This happens for the cases where the learning rates are too large and the algorithm to diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare plots\n",
    "fig, axis_list = plt.subplots(2, 3, figsize=(15, 10))\n",
    "n_iters = 5000\n",
    "alpha_list = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "## Iterate over the learning rates\n",
    "for i_alpha, alpha in enumerate(alpha_list):\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    theta, objectives_array_train, objectives_array_val = basic_gradient_decent(...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    ax = axis_list.flat[i_alpha]\n",
    "    ax.plot(objectives_array_train, label='Train')\n",
    "    ax.plot(objectives_array_val, label='Validation')\n",
    "    ax.set_title('$\\\\alpha$={:g}\\n'\n",
    "                 'Final train result={:g}\\n'\n",
    "                 'Optimal validation result={:g}\\n'\n",
    "                 'Num. of steps till optimal result: {}'.format(alpha,\n",
    "                                                                objectives_array_train[-1],\n",
    "                                                                objectives_array_val.min(),\n",
    "                                                                np.argmin(objectives_array_val) + 1))\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Objective')\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.legend()\n",
    "plt.tight_layout(rect=(0, 0, 0.95, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Hyper-Parameters\n",
    "\n",
    "We can see the overfitting effect, which at some point starts to cause the validation objective to degrade (increase). We would, therefore, like to pick out the hyper-parameters, which are the learning rate and the number of steps, which produce the best validation objective.\n",
    "\n",
    "We will select the hyper-parameters, according to these 6 runs, and re-run the algorithm using these hyper-parameters.\n",
    "\n",
    "(In practice the common way to avoid re-running the algorithm is to always store, during the run, the set of parameters ($\\Theta^{\\left(k\\right)}$) which have produced the best validation objective so far.\n",
    "\n",
    "#### Why can't we simply stop the algorithm when the validation objective to start to increase?\n",
    "\n",
    "In general, the objective curves will not be so smooth, such as in this case, and therefore, it will be hard to catch the exact point in which where the degradation starts. We will only be able to do so in retrospective. Some reasons for why the curve can be \"noisy\":\n",
    "- Stochastic gradient descent algorithms are noisy by nature.\n",
    "- We have a more complex objective function.\n",
    "- Gradient descent algorithm which uses momentum terms can sometimes go uphill before they continue to descent.\n",
    "- Some other reasons...\n",
    "\n",
    "✍️ Complete the following code to select the optimal learning rate and number on steps and re-run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "alpha = ...\n",
    "n_iters = ...\n",
    "theta, objectives_array_train, objectives_array_val = basic_gradient_decent(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('$\\\\alpha$={:g}\\n'\n",
    "             'Final train result={:g}\\n'\n",
    "             'Optimal validation result={:g}\\n'\n",
    "             'Num. of steps till optimal result: {}'.format(alpha,\n",
    "                                                            objectives_array_train[-1],\n",
    "                                                            objectives_array_val.min(),\n",
    "                                                            np.argmin(objectives_array_val) + 1))\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.set_ylim(0, np.median(objectives_array_val) * 3)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making predictions\n",
    "\n",
    "After learning the probability function $p\\left(y|\\boldsymbol{x};\\Theta\\right)$, we would like to make a prediction based on:\n",
    "\n",
    "$$\n",
    "\\hat{y}=\\underset{y}{\\arg\\max}\\ p\\left(y|\\boldsymbol{x};\\Theta\\right)=\\underset{k}{\\arg\\max}\\frac{e^{\\boldsymbol{\\theta}_{k}^T\\boldsymbol{x}}}{\\sum_{k'} e^{\\boldsymbol{\\theta}_{k'}^T\\boldsymbol{x}}}=\\underset{k}{\\arg\\max}\\ \\boldsymbol{\\theta}_{k}^T\\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "✍️ Complete the following code to calculate the prediction for a given feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    \"\"\"\n",
    "    Predict y_hat\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        The N x D features matrix.\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: ndarray\n",
    "        The 1D array of length N of predicted labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "y_hat_test = predict(features_test, theta)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of missclassifications) on the test set is: {}'.format(empirical_risk_test))\n",
    "\n",
    "## Plot estimation\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x_test[i], cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[y_hat_test[i]].split()[-1],\n",
    "                   color='black' if y_hat_test[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Add an $L2$ regularization term to the optimization process to reduce the overfitting. The objective function will now be:\n",
    "$$\n",
    "\\tilde{g}\\left(\\Theta;X,\\boldsymbol{y}\\right)=g\\left(\\Theta;X,\\boldsymbol{y}\\right)+\\lambda\\underbrace{\\left\\lVert\\Theta\\right\\rVert_2^2}_{=\\sum_{k,j}\\theta_{k,j}^2}\n",
    "$$\n",
    "\n",
    "(Commonly the regularization term should not include the last column of $\\Theta$, which is the bias term, multiplying the constant in the features matrix. We will ignore the fact here)\n",
    "\n",
    "What is the gradient of the new objective function?\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\tilde{g}\\left(\\Theta;X,\\boldsymbol{y}\\right)=\\nabla_\\Theta g\\left(\\Theta;X,\\boldsymbol{y}\\right)+?\n",
    "$$\n",
    "\n",
    "Therefore the gradient descent update step will now be:\n",
    "$$\n",
    "\\Theta^{\\left(k+1\\right)}=\\Theta^{\\left(k\\right)}-\\alpha\\nabla_\\Theta g\\left(\\Theta^{\\left(k\\right)}\\right)-?\n",
    "$$\n",
    "\n",
    "✍️ Complete the code below to implement a gradient descent with an $L2$ regularization.\n",
    "- Use the exact same code you used for **basic_gradient_decent** and only make changes to the update step. Use the original objective $g$ (not $\\tilde{g}$) to calculate the intermediate results **objectives_array_train** and **objectives_array_validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_with_l2(g, grad_g, alpha, n_iters, x_train, y_train, x_val, y_val, llambda):\n",
    "    \"\"\"\n",
    "    The gradient of the objective function.\n",
    "\n",
    "    Using:\n",
    "    - N: The number of samples.\n",
    "    - D: The number of features (n_pca_componens + 1).\n",
    "    - K: The length of every q vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h: funcion\n",
    "        The objective function.\n",
    "    grad_h: funcion\n",
    "        The gradient of the objective function.\n",
    "    alpha: float\n",
    "        The learning rate.\n",
    "    n_iters:\n",
    "        The number of steps to take.\n",
    "    x_train: ndarray\n",
    "        The N x D features matrix of the train set.\n",
    "    y_train: ndarray\n",
    "        The 1D array of length N of labels of the validation set.\n",
    "    x_val: ndarray\n",
    "        The N x D features matrix of the train set.\n",
    "    y_val: ndarray\n",
    "        The 1D array of length N of labels of the validation set.\n",
    "    llambda: float\n",
    "        The regularization coefficient.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta: ndarray\n",
    "        The K x D parameters matrix which the algorithm produces.\n",
    "    objectives_array_train: ndarray\n",
    "        A 1D array of length n_iters which contains the value of the objective \n",
    "        at each step evaluated on the train set.\n",
    "    objectives_array_train: ndarray\n",
    "        A 1D array of length n_iters which contains the value of the objective \n",
    "        at each step evaluated on the validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_classes = max(y_train) + 1\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return theta, objectives_array_train, objectives_array_val\n",
    "\n",
    "alpha = 1e-5\n",
    "n_iters = 500\n",
    "llambda = 1e3\n",
    "theta, objectives_array_train, objectives_array_val = gradient_decent_with_l2(g, grad_g, alpha, n_iters, features_train, y_train, features_val, y_val, llambda)\n",
    "\n",
    "## Plot the objective\n",
    "## ==================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('Otimization objective')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Hyper-Parameters\n",
    "\n",
    "The code below calculates and plots the results for $\\alpha=10^{-5},10^{-6}$ and $\\lambda=10^{4},10^{2},10^{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare plots\n",
    "fig, axis_list = plt.subplots(2, 3, figsize=(15, 10))\n",
    "n_iters = 5000\n",
    "alpha_list = [1e-5, 1e-6]\n",
    "lambda_list = [1e4, 1e2, 1e0]\n",
    "\n",
    "## Iterate over the learning rates\n",
    "for i_alpha, alpha in enumerate(alpha_list):\n",
    "    for i_lambda, llambda in enumerate(lambda_list):\n",
    "    \n",
    "        theta, objectives_array_train, objectives_array_val = gradient_decent_with_l2(g, grad_g, alpha, n_iters, features_train, y_train, features_val, y_val, llambda)\n",
    "\n",
    "        ax = axis_list[i_alpha][i_lambda]\n",
    "        ax.plot(objectives_array_train, label='Train')\n",
    "        ax.plot(objectives_array_val, label='Validation')\n",
    "        ax.set_title('$\\\\alpha$={:g}\\n\n",
    "                      $\\\\lambda$={:g}\\n\n",
    "                      Final train result={:g}\\n\n",
    "                      Optimal validation result={:g}\\n\n",
    "                      Num. of steps till optimal result: {}'.format(alpha,\n",
    "                                                                    llambda,\n",
    "                                                                    objectives_array_train[-1],\n",
    "                                                                    objectives_array_val.min(),\n",
    "                                                                    np.argmin(objectives_array_val) + 1))\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel('Objective')\n",
    "        ax.set_ylim(0, 5)\n",
    "        ax.legend()\n",
    "plt.tight_layout(rect=(0, 0, 0.95, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get that:\n",
    "- For $\\lambda=10^{4}$ the regularization is too strong and prevents the train objective form reaching a low objective value. This indeed prevents the validation objective from increasing, but it does not produce a good result.\n",
    "- For $\\lambda=10^{2}$ the regularization is able to significantly decrease the degradation of the validation objective and is even able to slightly improve the minimal value which the optimal validation objective is able to achieve. Sadly this is only results in a very small improvement.\n",
    "- For $\\lambda=10^{0}$ the regularization has almost no effect on the optimization process.\n",
    "\n",
    "Preventing the validation objective from degrading is usually important as well as improving the minimal validation objective as it typically results in a more stable solution.\n",
    "\n",
    "✍️ Select the optimal hyper-parameters and re-run the following code to calculate $\\Theta$ and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "alpha = ...\n",
    "n_iters = ...\n",
    "llambda = ...\n",
    "theta, objectives_array_train, objectives_array_val = gradient_decent_with_l2(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(objectives_array_train, label='Train')\n",
    "ax.plot(objectives_array_val, label='Validation')\n",
    "ax.set_title('$\\\\alpha$={:g}\\n\n",
    "              $\\\\lambda$={:g}\\n\n",
    "              Final train result={:g}\\n\n",
    "              Optimal validation result={:g}\\n\n",
    "              Num. of steps till optimal result: {}'.format(alpha,\n",
    "                                                            llambda,\n",
    "                                                            objectives_array_train[-1],\n",
    "                                                            objectives_array_val.min(),\n",
    "                                                            np.argmin(objectives_array_val) + 1))\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_xlabel('Objective')\n",
    "ax.set_ylim(0, np.median(objectives_array_val) * 3)\n",
    "ax.legend();\n",
    "\n",
    "## Calculate the test risk\n",
    "y_hat_test = predict(features_test, theta)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of missclassifications) on the test set is: {}'.format(empirical_risk_test))\n",
    "\n",
    "## Plot estimation\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x_test[i], cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[y_hat_test[i]].split()[-1],\n",
    "                   color='black' if y_hat_test[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: Submission\n",
    "\n",
    "To submit your code, download it as a **ipynb** file from Colab, and upload it to the course's website. You can download this code by selecting **Download .ipynb** from the **file** menu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
