{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework assignment 3 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the K-NN and LDA methods for face recognition.\n",
    "\n",
    "We will use PCA for generating features for our classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminders\n",
    "\n",
    "- Start by making a copy of this notebook in order to be able to save it.\n",
    "- Use **Ctrl+[** to expend all cells.\n",
    "\n",
    "## Tip of the day - Displaying functions documentation\n",
    "\n",
    "You can quickly display a function's documentation by pressing **Alt+/** when standing on it with the cursor.\n",
    "\n",
    "You can also open a small documentation window at the bottom of the screen by running a command for the format of **?{function}** in a new cell (and replacing **{function}** with your function's name.\n",
    "\n",
    "Try opening a new cell, bellow this one by clicking on the **+code** button below the menu bar. Then type:\n",
    "```python\n",
    "?print\n",
    "```\n",
    "into it and run it.\n",
    "\n",
    "You would need to use the functions' full call string. For example, to view the documentation of the **randint** function in the numpy package, you will have to run *?np.random.randint*. You can, of course, only view the documentation for this function after importing the numpy library (i.e., after running *import numpy as np*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your IDs\n",
    "\n",
    "‚úçÔ∏è Fill in your IDs in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "# Replace the IDs bellow with our own\n",
    "student1_id = '012345678'\n",
    "student2_id = '012345678'\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('Hello ' + student1_id + ' & ' + student2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "Importing the NumPy, Pandas and Matplotlib packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## This line makes matplotlib plot the figures inside the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "## Set some default values of the the matplotlib plots\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
    "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled faces in the wild\n",
    "\n",
    "For this task, we will use a dataset called [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/), which contains 13233 images of faces which belong to  5749 people. Each image in the dataset is labeled with a number corresponding to a person's name. All the images in the dataset are cropped and resized to the same image size.\n",
    "\n",
    "To load the data, we will use the scikit-learn's function [sklearn.datasets.fetch_lfw_people](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html). To make our life a bit easier we will only use faces of people which appear in the dataset more than 50 times. We can do so using the function's **min_faces_per_person** argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "dataset = fetch_lfw_people(min_faces_per_person=50)\n",
    "\n",
    "x = dataset.images\n",
    "y = dataset.target\n",
    "label_to_name_mapping = dataset.target_names\n",
    "image_shape = x[0].shape\n",
    "\n",
    "print('Number of images in the dataset: {}'.format(len(x)))\n",
    "print('Number of different persons in the dataset: {}'.format(len(np.unique(y))))\n",
    "print('Each images size is: {}'.format(image_shape))\n",
    "\n",
    "_, images_per_class = np.unique(y, return_counts=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(label_to_name_mapping, images_per_class)\n",
    "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
    "ax.set_title('Images per person')\n",
    "ax.set_ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the first 20 images in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x[i], cmap='gray')\n",
    "    ax.set_ylabel(label_to_name_mapping[y[i]])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task here is to be able to predict the correct label (name of the person) given an image of his face. Formally, we would like to find a classifier $h\\left(\\boldsymbol{x}\\right)$, which would minimize the misclassification rate:\n",
    "$$\n",
    "R\\left\\{h\\right\\}=E\\left[I\\left\\{h\\left(\\boldsymbol{x}\\right)\\neq y\\right\\}\\right]\n",
    "$$\n",
    "\n",
    "Here $\\boldsymbol{x}$ is the measured data, which is in our case the images, and $y$ is the label of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Splitting the Data\n",
    "\n",
    "‚úçÔ∏è Complete the code below to split the data into 80% train set and 20% test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Split the indices into 80% train / 20% test\n",
    "\n",
    "## Create a random generator using a fixed seed\n",
    "rand_gen = ...\n",
    "\n",
    "...\n",
    "\n",
    "x_train = ...\n",
    "x_test = ...\n",
    "\n",
    "y_train = ...\n",
    "y_test = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('Number of training sample: {}'.format(n_samples_train))\n",
    "print('Number of test sample: {}'.format(n_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for compression feature selection\n",
    "\n",
    "Instead of working directly with the pixels as our input, we would like to select a smaller number of features to use as an input to our classifier.\n",
    "\n",
    "We will use PCA to represent a given image using a smaller number of variables. \n",
    "\n",
    "We can also think of this task as trying to compress the image representation.\n",
    "\n",
    "Currently, each image is represented using 2914 numbers (47 x 62 pixels). Let us try to reduce this number using PCA. This, of course, will come at the cost of not being able to reconstruct the image exactly, but only approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the data\n",
    "\n",
    "In order to use PCA on the images need to store each image as a vector. We will reshape each image to be a 1d vector of size 2914 x 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat_train = x_train.reshape(len(x_train), -1) ## Reshape the training data to (n_smaples_train x 2914)\n",
    "x_flat_test = x_test.reshape(len(x_test), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the mean value\n",
    "\n",
    "We will start by calculating the data's mean and remove it form the data.\n",
    "\n",
    "**Note**: Another common practice before performing PCA is to normalize the data by dividing by the standard deviation of the data. This is mainly important for datasets which include multiple types of data which can even be measured in different units. In our case all the values feed into the PCA are pixels with values between 0 and 255, therefore this normalization is less needed in this case and can even be harmful.  \n",
    "\n",
    "‚úçÔ∏è Complete the code below to compute and remove the data's mean.\n",
    "\n",
    "- Complete the mean using the train set ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "mean_x = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "\n",
    "## Ploting the mean images\n",
    "mean_image = mean_x.reshape(image_shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(mean_image, cmap='gray')\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.set_title('The mean face');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the PCA basis\n",
    "\n",
    "‚úçÔ∏è Complete the code below to compute the basis vectors of PCA.\n",
    "\n",
    "- Implement PCA using the matrix product operator, **A @ B**, and the function [numpy.linalg.eig](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) which calucates the eigen-values and eigen-vectors of a matrix:\n",
    "```python\n",
    "D, V = np.linalg.eig(A)\n",
    "```\n",
    "where **D** is the 1D array of the eign-values and **V** is the unitary matrix of the eigen-vectors. **A = V @ np.diag(D) @ V.T**\n",
    "\n",
    "- Do not use any function which implements PCA directly.\n",
    "- Remember to subtract the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "...\n",
    "\n",
    "basis = ...  # The PCA basis. this should be a (2914 x 2914) matrix with a basis vector on each column.\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%\n",
    "\n",
    "## plot the first 20 PCA basis vectors\n",
    "fig, ax_array = plt.subplots(4, 5, figsize=(10,7))\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    img = basis[:, i].reshape(*image_shape)  ## Reshaping the basis vectors bask to images\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('e{}'.format(i))\n",
    "fig.suptitle('PCA Basis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you get something that  roughly resembles faces in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the PCA components\n",
    "\n",
    "‚úçÔ∏è Complete the function below which calculates the first n_components of a given image using the calculated basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_principal_components(img, basis, mean_x, n_components):\n",
    "    \"\"\"\n",
    "    Reconstruct an image from it's principal componants given a basis.\n",
    "\n",
    "    Using:\n",
    "    - D: the dimension of the original space (number of pixels).\n",
    "    - H: the images' height\n",
    "    - W: the images' width\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    img: ndarray\n",
    "        The HxW 2D array of the input image.\n",
    "    basis: ndarray\n",
    "        The DxD 2D array of the PCA basis where each column is a basis vector.\n",
    "    mean_x\n",
    "        The 1D array of length (HxW) of the mean data value.\n",
    "    n_components: int\n",
    "        The number of principal components to return\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    components: ndarray\n",
    "        The 1D array of length n_components of the principal components.\n",
    "    \"\"\"\n",
    "    img_flat = img.flat\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    components = ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    return components\n",
    "\n",
    "print('The 10 principal components of the first test image are:\\n{}'.format(list(calc_principal_components(x_test[0], basis, mean_x, 10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing an image from its principal components\n",
    "\n",
    "‚úçÔ∏è Fill in the function bellow which reconstructs an image from its principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(components, basis, mean_x, image_shape):\n",
    "    \"\"\"\n",
    "    Reconstruct an image from it's principal componants given a basis.\n",
    "\n",
    "    Using:\n",
    "    - N: number of components.\n",
    "    - D: the dimension of the original space (number of pixels).\n",
    "    - H: the images' height\n",
    "    - W: the images' width\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    components: ndarray\n",
    "        The 1D array of length D of the image's components.\n",
    "    basis: ndarray\n",
    "        The DxD 2D array of the PCA basis where each column is a basis vector.\n",
    "    mean_x\n",
    "        The 1D array of length (HxW) of the mean data value.\n",
    "    image_shape: list\n",
    "        The list: [image_hight, image_width]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img: ndarray\n",
    "        The HxW 2D array of the reconstructed image.\n",
    "    \"\"\"\n",
    "\n",
    "    n_components = len(components)\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    img_flat = ...\n",
    "    img = ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    return img\n",
    "\n",
    "## Ploting the reconstruction of the first 10 test images\n",
    "fig, ax_array = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    components = calc_principal_components(x_test[i], basis, mean_x, 300)\n",
    "    reconstructed_image = reconstruct_image(components, basis, mean_x, image_shape)\n",
    "\n",
    "    ax_array[0][i].imshow(x_test[i], cmap='gray')\n",
    "    ax_array[0][i].set_yticks([])\n",
    "    ax_array[0][i].set_xticks([])\n",
    "\n",
    "    ax_array[1][i].imshow(reconstructed_image, cmap='gray')\n",
    "    ax_array[1][i].set_yticks([])\n",
    "    ax_array[1][i].set_xticks([])\n",
    "ax_array[0][0].set_ylabel('Original')\n",
    "ax_array[1][0].set_ylabel('Reconstructed')\n",
    "fig.suptitle('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the reconstructed image and error as a function of the number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For visualization, find the index of the first image of Ariel Sharon in the test set\n",
    "ariel_sharon_index = np.argwhere(y_test == 0)[0, 0]\n",
    "\n",
    "n_components_list = []\n",
    "reconstruction_error_list = []\n",
    "\n",
    "fig, ax_array = plt.subplots(4, 5, figsize=(10,7))\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    n_components = i * 20\n",
    "    components = calc_principal_components(x_test[ariel_sharon_index], basis, mean_x, n_components)\n",
    "    reconstructed_image = reconstruct_image(components, basis, mean_x, image_shape)\n",
    "    \n",
    "    reconstruction_error = ((reconstructed_image - x_test[ariel_sharon_index]) ** 2).mean() ** 0.5\n",
    "    reconstruction_error_list.append(reconstruction_error)\n",
    "    n_components_list.append(n_components)\n",
    "    \n",
    "    ax.imshow(reconstructed_image, cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('{} components'.format(n_components))\n",
    "fig.suptitle('Reconstruction vs. Number of components')\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_components_list, reconstruction_error_list)\n",
    "ax.set_title('RMSE vs. number of components')\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we shall use 300 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.decomposition.PCA\n",
    "\n",
    "From here on we will use the [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to perform PCA.\n",
    "\n",
    "‚úçÔ∏è Read the function's documentation and complete the following code to run PCA using scikit-learn's class. Calculate the basis, the test image's components, and the reconstruction of the test images.\n",
    "\n",
    "- Note that the basis is only define up to a $\\pm$ sign there the basis can have a different sign then the ones calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 300\n",
    "pca = PCA(n_components)\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Calculate the PCA basis\n",
    "pca.fit(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## plot first 20 PCA basis vectors\n",
    "fig, ax_array = plt.subplots(4, 5, figsize=(10,7))\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    img = pca.components_[i].reshape(*image_shape)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('e{}'.format(i))\n",
    "fig.suptitle('PCA Basis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Calculate the componantes of all the test images.\n",
    "components = pca.transform(...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "print('The 10 principal components of the first test image are:\\n{}'.format(list(components[0, :10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "## Reconstruct all the test images form their componants.\n",
    "reconstructed_images_flat = pca.inverse_transform(...\n",
    "reconstructed_images = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "## Ploting the reconstruction of the first 10 test images\n",
    "fig, ax_array = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    ax_array[0][i].imshow(x_test[i], cmap='gray')\n",
    "    ax_array[0][i].set_yticks([])\n",
    "    ax_array[0][i].set_xticks([])\n",
    "\n",
    "    ax_array[1][i].imshow(reconstructed_images[i], cmap='gray')\n",
    "    ax_array[1][i].set_yticks([])\n",
    "    ax_array[1][i].set_xticks([])\n",
    "ax_array[0][0].set_ylabel('Original')\n",
    "ax_array[1][0].set_ylabel('Reconstructed')\n",
    "fig.suptitle('Reconstructed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-NN Classification\n",
    "\n",
    "Lets us now try to classify the images using 1-nearest neighbor (1-NN). \n",
    "\n",
    "‚úçÔ∏è Complete the following code to implement the 1-NN classification.\n",
    "\n",
    "- Use the [cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) function, which we encountered in the last assignment, to calculate the matrix of all distances between two sets of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "features_train = pca.transform(x_flat_train)\n",
    "features_test = pca.transform(x_flat_test)\n",
    "\n",
    "def one_nn(features, stored_features, stored_y):\n",
    "    \"\"\"\n",
    "    Calculates the estimated labels for a given set of features using the 1-NN method. \n",
    "\n",
    "    Using:\n",
    "    - N: the number of samples in the train set.\n",
    "    - M: the number of samples for which the labels are to be estimated.\n",
    "    - D: the length of the feature vectors.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: ndarray\n",
    "        The MxD 2D array of features for which the labels are to be estimated.\n",
    "    stroed_features: ndarray\n",
    "        The NxD 2D array of the features of the train set.\n",
    "    stored_y: ndarray\n",
    "        The 1D array of length N of the labels of the train set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: ndarray\n",
    "        The 1D array of length M of the estimated labels.\n",
    "    \"\"\"\n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "\n",
    "    y_hat = ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "y_hat_test = one_nn(features_test, features_train, y_train)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an empirical risk of about $0.5\\pm0.1$ on the test set. \n",
    "\n",
    "The following code displays the estimated labels of the first 20 faces in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot estimation\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x_test[i], cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[y_hat_test[i]].split()[-1],\n",
    "                   color='black' if y_hat_test[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "The class [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) implements the K-NN algorithm.\n",
    "\n",
    "‚úçÔ∏è Read the function's documentation and fill in the following code run 1-NN using scikit-learn's class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "classifier = KNeighborsClassifier(...\n",
    "classifier.fit(...\n",
    "\n",
    "y_hat_test = classifier...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "Let us now evaluate the linear discrimination analysis (LDA) method.\n",
    "\n",
    "## Learning\n",
    "\n",
    "First, we will estimate the model's parameters. \n",
    "\n",
    "‚úçÔ∏è Fill in the code below to calculate the model's parameters using MLE.\n",
    "\n",
    "Reminder, LDA's model parameters are:\n",
    "- The mean values for each class\n",
    "- The covariance matrix for all classes.\n",
    "- The prior distribution of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lda_parameters(features, y):\n",
    "    \"\"\"\n",
    "    Calculates the parameters of the LDA model. \n",
    "\n",
    "    Using:\n",
    "    - N: the number of samples in the train set.\n",
    "    - D: the length of the feature vectors.\n",
    "    - K: the number of classes.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: ndarray\n",
    "        The NxD 2D array of features the train set.\n",
    "    y: ndarray\n",
    "        The 1D array of length N of the labels of the train set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    py: ndarray\n",
    "        The 1D array of length K of the prior probability of each class: P(Y=k).\n",
    "    mu: ndarray:\n",
    "        The KxD array of the K means of each classes distribution.\n",
    "    cov_mat: ndarray:\n",
    "        The DxD array of the covariance matrix of the classes distribution.\n",
    "    \"\"\"\n",
    "    n_classes = np.max(y) + 1\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return py, mu, cov_mat\n",
    "\n",
    "\n",
    "py, mu, cov_mat = calc_lda_parameters(features_train, y_train)\n",
    "\n",
    "## Plot P(y)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(label_to_name_mapping, py)\n",
    "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
    "ax.set_title('$p\\\\left(Y=k\\\\right)$')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "## Display the means\n",
    "fig, ax_array = plt.subplots(3, 4)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    img_flat = pca.inverse_transform(mu[i][None, :])[0]\n",
    "    img = img_flat.reshape(image_shape)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[i].split()[-1])\n",
    "fig.suptitle('Mean image', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying\n",
    "\n",
    "Now we can use the estimated parameters to build our classifier.\n",
    "\n",
    "Reminder: in the case of multiple classes the classification of the LDA model is given by:\n",
    "\n",
    "$$\n",
    "h\\left(\\boldsymbol{x}\\right)=\\underset{k}{\\arg\\max}\\ p\\left(\\boldsymbol{x}|Y=k\\right)p\\left(Y=k\\right)\n",
    "$$\n",
    "\n",
    "The argmax in solved simply be testing all $k$'s.\n",
    "\n",
    "‚úçÔ∏è  Complete the following code to implement the classification function:\n",
    "\n",
    "- *Optional*: The term $\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)^T\\Sigma^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)$, which appears in $p\\left(\\boldsymbol{x}|y\\right)$, is called the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) between $\\boldsymbol{x}$ and $\\boldsymbol{\\mu}$ based on the covariance matrix $\\Sigma$. You can use the **cdist** function to calculate all the Mahalanobis distances between a set of $\\boldsymbol{x}$'s and a set of $\\boldsymbol{\\mu}$'s. This can be done using the **'mahalanobis'** metric and adding a **VI = $\\Sigma^{-1}$** arguments to the **cdist** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_classify(features, py, mu, cov_mat):\n",
    "    \"\"\"\n",
    "    Calculates the parameters of the LDA model. \n",
    "\n",
    "    Using:\n",
    "    - N: the number of samples in the train set.\n",
    "    - D: the length of the feature vectors.\n",
    "    - K: the number of classes.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: ndarray\n",
    "        The NxD 2D array of features for which the labels are to be estimated.\n",
    "    py: ndarray\n",
    "        The 1D array of length K of the prior probability of each class: P(Y=k).\n",
    "    mu: ndarray:\n",
    "        The KxD array of the K means of each classes distribution.\n",
    "    cov_mat: ndarray:\n",
    "        The DxD array of the covariance matrix of the classes distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: ndarray\n",
    "        The 1D array of length N of the estimated labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "    ...\n",
    "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "y_hat_test = lda_classify(features_test, py, mu, cov_mat)\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of missclassifications) on the test set is: {}'.format(empirical_risk_test))\n",
    "\n",
    "## Plot estimation\n",
    "fig, ax_array = plt.subplots(4, 5)\n",
    "for i, ax in enumerate(ax_array.flat):\n",
    "    ax.imshow(x_test[i], cmap='gray')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(label_to_name_mapping[y_hat_test[i]].split()[-1],\n",
    "                   color='black' if y_hat_test[i] == y_test[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
    "\n",
    "The class [sklearn.discriminant_analysis.LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) implements the LDA algorithm.\n",
    "\n",
    "‚úçÔ∏è Read the function's documentation and fill in the following code run LDA using scikit-learn's class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
    "...\n",
    "y_hat_test = ...\n",
    "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
    "\n",
    "empirical_risk_test = (y_hat_test != y_test).mean()\n",
    "\n",
    "print('The empirical risk (amount of misclassifications) on the test set is: {}'.format(empirical_risk_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: Submission\n",
    "\n",
    "To submit your code, download it as a **ipynb** file from Colab, and upload it to the course's website. You can download this code by selecting **Download .ipynb** from the **file** menu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
