{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Notations\n",
    "\n",
    "- **Scalar**: Non-bold lower case letter. Example $y$\n",
    "- **Column vector**: Bold lower case letter. Example $\\boldsymbol{x}$\n",
    "- **Matrix**: Upper case letter. Example $A$\n",
    "- **A set of scalars/vectors**: Curly brackets. Example $\\left\\{\\boldsymbol{x}_i\\right\\}$ or $\\left\\{y_i\\right\\}$\n",
    "- **An element of a vector, a matrix or a set**: Subscript index. Example $x_i$ or $a_{i,j}$\n",
    "- **The value in an iterative process**: Superscript index in parentheses. Example $\\theta^{\\left(t\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Notations\n",
    "\n",
    "- **A single random sample**: $\\omega$\n",
    "- **A random variable (RV)**: $X\\left(\\omega\\right)$ (usually shorten to $X$. Note that capital letters are also used for matrices)\n",
    "- **A realization of an RV**: $x=X\\left(\\omega\\right)$\n",
    "- **The probability of an event** $Pr\\left(0.2<X<0.6\\right)$\n",
    "- **Probability Mass Function (PMF)**: (for a discrete RV) $p_X\\left(x\\right)=Pr\\left(X=x\\right)$\n",
    "- **Cumulative Distribution Function (CDF)**: $F_X\\left(x\\right)=Pr\\left(X\\leq x\\right)$\n",
    "- **Probability Density Function (PDF)**: $f_X\\left(x\\right)$ define by $F_X\\left(x\\right)=\\int_{-\\infty}^xf_Xdx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common notation to describe a dataset:\n",
    "\n",
    "- $N$ - The number of samples in a dataset.\n",
    "- $d$ The dimension of the data.\n",
    "- $\\boldsymbol{x}$ - A vector of measured values related to a single sample.\n",
    "- $\\left\\{\\boldsymbol{x}_i\\right\\}$ - The set of measurements in a dataset.\n",
    "- $X$ - The matrix of measurements $\\left[\\boldsymbol{x}_0,\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_{N-1}\\right]^T$.\n",
    "\n",
    " (Note that $X$ is also sometime used as the random variable generating $\\boldsymbol{x}$).\n",
    "- $x_{i,j}$ - The $j$-th element of the $i$-th measurement (which is the $\\left(i,j\\right)$-th element of the matrix $X$).\n",
    "- $\\left\\{y_i\\right\\}$ - The set of labels in a dataset.\n",
    "- $\\boldsymbol{y}$ - the vector of labels $\\left[y_0,y_1,\\ldots,y_{N-1}\\right]^T$.\n",
    "- $y_i$ - The $i$-th label (which is the $i$-th element of the vector $\\boldsymbol{y}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions for a Known Distribution\n",
    "\n",
    "The problem of coming up with a mapping $\\hat{y}=h\\left(\\boldsymbol{x}\\right)$ which is optimal under some criteria, based on the joint distribution $f_{X,Y}\\left(\\boldsymbol{x},y\\right)$.\n",
    "\n",
    "- **Classification**: $y$ can only take a finite set of discrete values.\n",
    "- **Regression**: $y$ can take values from a continues range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Common Criteria - Minimal Expected Loss\n",
    "\n",
    "Minimizing the **risk**/**cost** function $R$, which is defined as defined as the expectation value of some loss function $l$:\n",
    "\n",
    "$$\n",
    "R\\left(h\\right) = E\\left[l\\left(h\\left(\\boldsymbol{x}\\right),y\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Optimization Problem - Searching for a Minimum\n",
    "\n",
    "In general, is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^*=\\underset{\\boldsymbol{x}}{\\arg\\min}\\ g\\left(\\boldsymbol{x}\\right)\n",
    "$$\n",
    "\n",
    "Find the value of $x$ which produces the minimal value of the objective function $g\\left(x\\right)$\n",
    "\n",
    "For the prediction problem:\n",
    "\n",
    "$$\n",
    "h^*=\\underset{h}{\\arg\\min}\\ R\\left(h\\right)=\\underset{h}{\\arg\\min}\\ E\\left[l\\left(h\\left(\\boldsymbol{x}\\right),y\\right)\\right]\n",
    "$$\n",
    "\n",
    "Finding an optimal function is hard. In the case of expected loss, the law of total expectation can be use to simplify the problem to:\n",
    "\n",
    "$$\n",
    "h^*\\left(\\boldsymbol{x}\\right)=\\underset{\\hat{y}}{\\arg\\min}\\ E_{Y|X}\\left[l\\left(\\hat{y},y\\right)\\right]\\\\\n",
    "$$\n",
    "\n",
    "Here we need to find for a given $x$ the optimal value $\\hat{y}$ which minimizes $E_{Y|X}\\left[l\\left(\\hat{y},y\\right)\\right]$\n",
    "\n",
    "Notice that $h^*$ depends only on $f_{Y|X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Loss Function and Their Optimal Predictors\n",
    "\n",
    "| Loss Name | Risk Name | Loss Function | Optimal Predictor |\n",
    "|-----------|-----------|---------------|-------------------|\n",
    "| Zero-One loss | Misclassification rate | $l\\left(y_1,y_2\\right)=I\\left\\{y_1\\neq y_2\\right\\}$ | $h^*\\left(x\\right)=\\underset{y}{\\arg\\max}\\ f_{Y\\mid X}\\left(y\\mid X=x\\right)$ |\n",
    "| L1 | Mean absolute<br>error| $l\\left(y_1,y_2\\right)=\\left\\vert y_1-y_2\\right\\vert$ | Median: $h^*\\left(x\\right)=\\hat{y}$<br>$s.t.\\ F_{Y\\mid X}\\left(\\hat{y}\\mid X=x\\right)=0.5$ |\n",
    "| L2 | Mean squared<br>error (MSE) |$l\\left(y_1,y_2\\right)=\\left(y_1-y_2\\right)^2$ | $h^*\\left(x\\right)=E_{Y\\mid X}\\left[y\\right]$ |\n",
    "\n",
    "Zero-one loss is usually used for classification problems and L1 & L2 for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Making Predictions for an Unknown Distribution Using a Dataset\n",
    "\n",
    "3 approaches for using the dataset to solve the problem:\n",
    "\n",
    "1. **The generative approach**: Use the dataset to infer the joint distribution $f_{X,Y}\\left(x,y\\right)$. Then use the joint distribution to find the optimal predictor $h^*$. In classification problems we will usually decompose $f_{X,Y}$ into $f_{X,Y}\\left(x,y\\right)=f_{X|Y}\\left(x|y\\right)f_Y\\left(y\\right)$\n",
    "\n",
    "2. **The type I discriminative approach**: Use the data to infer the conditional distribution $f_{Y|X}\\left(y|x\\right)$. Then use the conditional distribution to find the optimal predictor $h^*$.\n",
    "\n",
    "3. **The type II discriminative approach**: Use the data to directly find $h^*$ by using some direct method or by replacing the expectation value in the risk function with some empirical equivalent function and solving the optimization problem. For example, empirical risk minimization (ERM):\n",
    "\n",
    "$$\n",
    "h^*=\\underset{h}{\\arg\\min}\\ \\frac{1}{N}\\sum_il\\left(h\\left(x_i\\right),y_i\\right)\n",
    "$$\n",
    "\n",
    "We will usually find the $f_{X,Y}\\left(x,y\\right)$, $f_{Y|X}\\left(y|x\\right)$ or $h\\left(x_i\\right)$ by defining an solving an optimization problem of how well they fit the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "When looking for the optimal function $f_{X,Y}\\left(x,y\\right)$, $f_{Y|X}\\left(y|x\\right)$ or $h\\left(x_i\\right)$ in the range of all function, we are most likely find a solution which is extremely optimal for the specific dataset but is far from optimal for any other dataset drawn from the same distribution. We would like to find a solution which generalizes well, i.e. will produce reasonable results for any dataset drawn from the data's (usually unknown) distribution.\n",
    "\n",
    "#### Ways to Reduce Overfitting\n",
    "\n",
    "##### Restricting the Search Space\n",
    "\n",
    "Limit the space in which we look for a solution. The smaller space might not contain the optimal solution, but will hopefully contain a \"good enough\" approximation of it. One way to do so is by using a parametric family of functions. For example, we can limit $h$ to be a linear function of $x$.\n",
    "\n",
    "##### Regularization\n",
    "\n",
    "Add a penalty term to the optimization problem to encourage it to converge to a \"simpler\" solution. Common penalty terms are:\n",
    "- **Tikhonov Regularization**: adding the $L2$ norm of the parameters vector to the optimization. Encourages the algorithm to select a solution with smaller parameters.\n",
    "- **$L1$ Regularization**: adding the $L1$ norm of the parameters vector to the optimization. Encourages the algorithm to select a solution with smaller and sparser set of parameters.\n",
    "\n",
    "### The Bias vs. Variance Trade Off\n",
    "\n",
    "Since the data in randomly generated the resulting prediction function, which is calculated from the data is random as well. \n",
    "\n",
    "- **Bias**: The difference between the mean (over different datasets) of the prediction and the optimal one.\n",
    "- **Variance**: The variance of the prediction (over different datasets).\n",
    "\n",
    "Pushing/limiting the algorithm to a specific subset of solutions will usually result in a worse approximation of the optimal solution, and will increase the bias of the solution. But on the other hand, will make the algorithm less sensitive to the specific value of each single data point and therefore will reduce the variance. This is known as the bias vs. variance trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing and Using Features\n",
    "\n",
    "When selecting a model we can always change the input to the model from the measurements themselves to any set of functions of the measurements, $\\varphi_i\\left(\\boldsymbol{x}\\right)$, called features. \n",
    "\n",
    "The vector of measurements will be replace by the vector of features:\n",
    "$$\n",
    "\\boldsymbol{\\varphi}\\left(\\boldsymbol{x}\\right)=\\left[\\phi_0\\left(\\boldsymbol{x}\\right),\\phi_0\\left(\\boldsymbol{x}\\right),\\ldots,\\phi_{m-1}\\left(\\boldsymbol{x}\\right)\\right]^T \n",
    "$$\n",
    "\n",
    "And the measurements matrix will be replaced by the features matrix:\n",
    "\n",
    "$$\n",
    "\\Phi=\\left[\\boldsymbol{\\varphi}\\left(\\boldsymbol{x}_0\\right),\\boldsymbol{\\varphi}\\left(\\boldsymbol{x}_1\\right),\\ldots,\\boldsymbol{\\varphi}\\left(\\boldsymbol{x}_{N-1}\\right)\\right]^T\n",
    "$$\n",
    "\n",
    "An example: we take $h$ to be a linear function of the features and use the powers of $x$ up to the $10$-th order as our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing\n",
    "\n",
    "Replacing the measurements with a version of them with zero mean and STD of 1:\n",
    "\n",
    "$$\n",
    "\\varphi_j\\left(\\boldsymbol{x}\\right) = \\frac{x_j-\\bar{x}_j}{\\sigma_j}\\qquad\\bar{x}_j=\\frac{1}{N}\\sum_ix_{i,j}\\qquad\\sigma_j=\\sqrt{\\frac{1}{N}\\sum_i\\left(x_{i,j}-\\bar{x}_j\\right)^2}\n",
    "$$\n",
    "\n",
    "This is important when the data comes in arbitrary scale (or units) and the algorithm is sensitive to distances between points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    "\n",
    "One way of making the model \"simpler\" is to use a smaller but more meaningful set of input features.\n",
    "\n",
    "##### Principal Component Analysis (PCA)\n",
    "\n",
    "A linear projection of the data on to the $m$ most significant direction in the data space.\n",
    "\n",
    "The sample-covariance matrix:\n",
    "\n",
    "$$\n",
    "P=\\frac{1}{N}X^TX\n",
    "$$\n",
    "\n",
    "($P$ is a positive semi-definite (PSD) matrix)\n",
    "\n",
    "The eigen-decomposition of $P$:\n",
    "\n",
    "$$\n",
    "P=V\\Lambda V^T\n",
    "$$\n",
    "\n",
    "Where $V$ is the unitary matrix with the eigen-vectors, $\\boldsymbol{v}_i$, of $P$ on each column and $\\Lambda$ is the a diagonal matrix with the eigen-values on the diagonal sorted from large to small.\n",
    "\n",
    "The PCA projection is performed by projecting a measurements vector onto the $m$ first columns of $V$ (the eiven-vector which correspond to the largest eigen-values).\n",
    "\n",
    "$$\n",
    "\\varphi_i\\left(x\\right) = \\boldsymbol{v}_i^Tx\\qquad i\\in\\left[0,\\ldots,m-1\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Hyper-Parameters\n",
    "\n",
    "Usually an algorithm will have some parameters which can be optimally selected using an analytic or numeric method, and some parameter which will have to be selected by running a grid search or a by trail and error. These parameters are refer to to as **hyper-parameters**.\n",
    "\n",
    "#### Validation Set\n",
    "\n",
    "Evaluation of the hyper-parameters is usually done on a validation set different then the train and the test sets.\n",
    "\n",
    "#### Cross Validation \n",
    "\n",
    "Instead of using a designated validation-set we can divide the train set into $K$ subsets and repeat the evaluation $K$ times, each time using a different subset as the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating PMF\n",
    "\n",
    "For discrete RV the PMF can be estimated by:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{X}\\left(x\\right)=\\tfrac{1}{N}\\sum_{i=1}^N I\\left\\{x_i = x\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Empirical Cumulative Distribution Function (the ECDF)\n",
    "\n",
    "The ECDF is an estimation of the CDF and is given by:\n",
    "\n",
    "$$\n",
    "\\hat{F}_{X}\\left(x\\right)=\\tfrac{1}{N}\\sum_{i=1}^N I\\left\\{x_i \\leq x\\right\\}\n",
    "$$\n",
    "\n",
    "The ECDF results in a non-continuous CDF which is a sum of step functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "\n",
    "The histogram is an estimation of the PDF and is given by:\n",
    "\n",
    "- $l_k$, $r_k$ - The left and right edges of the $k$'s bin.\n",
    "\n",
    "$$\n",
    "h_X\\left(l_k \\leq x < r_k\\right) = \\tfrac{1}{N\\cdot\\left(r_k-l_k\\right)}\\sum_{i=1}^N I\\left\\{l_k \\leq x_i < r_k\\right\\}\n",
    "$$\n",
    "\n",
    "A common rule of thumb for selecting the bins is to divide the range of values into $\\sqrt{N}$ equal bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimation (KDE)\n",
    "\n",
    "KDE is an estimation of the PDF and is given by:\n",
    "\n",
    "- $\\phi\\left(x\\right)$ - the smoothing Parzen window.\n",
    "\n",
    "$$\n",
    "\\hat{f}_{\\phi,h,X}\\left(x\\right) = \\frac{1}{N\\cdot h}\\sum_{i=1}^N \\phi\\left(\\frac{x-x_i}{h}\\right)\n",
    "$$\n",
    "\n",
    "Two commonly used Parzen windows are:\n",
    "\n",
    "- A Gaussian: $\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)$\n",
    "- A rectangular function $I\\left\\{\\left|x\\right|\\leq0.5\\right\\}$\n",
    "\n",
    "A rule of thumb for selecting the bandwidth $h$ for the Gaussian window is: $\\left(\\frac{4\\cdot\\text{std}\\left\\{x_i\\right\\}}{3N}\\right)^\\frac{1}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Estimation\n",
    "\n",
    "In parametric estimation we assume that $f$ has a known form up to some parameters $\\boldsymbol{\\theta}$. For example, a Gaussian with unknown mean and STD (in this case $\\boldsymbol{\\theta}=\\left[\\mu, \\sigma\\right]^T$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the parameters as random variables (the Bayesian point of view)\n",
    "\n",
    "##### Maximum A Posteriori Estimation\n",
    "\n",
    "We have an assumption on the a priori distribution $f_{\\boldsymbol{\\Theta}}\\left(\\boldsymbol{\\theta}\\right)$ of $\\boldsymbol{\\theta}$ and we would like to fine the most probable $\\boldsymbol{\\theta}$ given the data in the dataset. I.e. we would like to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^*=\\underset{\\boldsymbol{\\theta}}{\\arg\\max}\\ f_{\\boldsymbol{\\Theta}|\\boldsymbol{X}}\\left(\\boldsymbol{\\theta}|\\boldsymbol{x}\\right)=\\underset{\\boldsymbol{\\theta}}{\\arg\\max}\\ \n",
    "f_{\\boldsymbol{X}|\\boldsymbol{\\Theta}}\\left(\\boldsymbol{x}|\\boldsymbol{\\theta}\\right)\n",
    "f_{\\boldsymbol{\\Theta}}\\left(\\boldsymbol{\\theta}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the parameter as fixed unknowns (the Frequentist point of view)\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Here we have no prior assumption on $\\boldsymbol{\\theta}^*$ and we would like to find the most likely $\\boldsymbol{\\theta}^*$ given the data.\n",
    "\n",
    "- The likelyhood function is defined as: $\\mathcal{L}\\left(\\boldsymbol{\\theta};\\boldsymbol{x}\\right)=p_{\\boldsymbol{X}}\\left(\\boldsymbol{x}\\ ;\\boldsymbol{\\theta}\\right)$\n",
    "- The log-likelyhood is define as $l\\left(\\boldsymbol{\\theta};\\boldsymbol{x}\\right)=\\log\\left(\\mathcal{L}\\left(\\boldsymbol{\\theta};\\boldsymbol{x}\\right)\\right)$\n",
    "\n",
    "We would there for want to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^*\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\max}\\ \\mathcal{L}\\left(\\boldsymbol{\\theta};\\boldsymbol{x}\\right) \\\\\n",
    "& = \\underset{\\boldsymbol{\\theta}}{\\arg\\min}\\ -l\\left(\\boldsymbol{\\theta};\\boldsymbol{x}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Algorithms\n",
    "\n",
    "### Linear Least Squares (LLS, also known as ordinary least squares-OLS)\n",
    "\n",
    "- Solves: Regression problems\n",
    "- Approach: Discriminative type II\n",
    "- Model: $h\\left(x\\right)=\\boldsymbol{\\theta}^T\\boldsymbol{x}$\n",
    "- Is define of MSE risk\n",
    "\n",
    "Solves:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^* =\\underset{\\boldsymbol{\\theta}}{\\arg\\min}\\ \\left\\lVert X\\boldsymbol{\\theta}-\\boldsymbol{y}\\right\\rVert_2\n",
    "$$\n",
    "\n",
    "Has a closed form solution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^*=\\left(X^TX\\right)^{-1}X\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "#### Ridge Regression -  LLS with Tikhonov Regularization ($L2$)\n",
    "\n",
    "Solves:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^* \n",
    "= \\underset{\\boldsymbol{\\theta}}{\\arg\\min}\\ \\left\\lVert X\\boldsymbol{\\theta}-\\boldsymbol{y}\\right\\rVert_2^2 +\\lambda\\left\\lVert\\boldsymbol{\\theta}\\right\\rVert_2^2\n",
    "$$\n",
    "\n",
    "Has a closed form solution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}=\\left(X^TX+\\lambda I\\right)^{-1}X\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "#### Least Absolute Shrinkage and Selection Operator (LASSO) -  LLS with $L1$ Regularization \n",
    "\n",
    "Solves:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^* \n",
    "= \\underset{\\boldsymbol{\\theta}}{\\arg\\min}\\ \\left\\lVert X\\boldsymbol{\\theta}-\\boldsymbol{y}\\right\\rVert_2^2 +\\lambda\\left\\lVert\\boldsymbol{\\theta}\\right\\rVert_1\n",
    "$$\n",
    "\n",
    "Has a numerical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (K-NN)\n",
    "\n",
    "- Solves: Classification and regression problems\n",
    "- Approach: Discriminative type II\n",
    "- Non-Parametric Model\n",
    "- Hyper-Parameters: $K$\n",
    "\n",
    "$h\\left(\\boldsymbol{x}\\right)$ finds the $K$ euclidean nearest neighbors to $\\boldsymbol{x}$ form the training set, and return the mean value for regression, or the majority vote value for classification of the the labels of the $K$ neighbors.\n",
    "\n",
    "- Requires amount of data which is exponential in the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Solves: Classification problems\n",
    "- Approach: Generative\n",
    "- Model: $f_{X|Y}=\\frac{1}{\\sqrt{\\left(2\\pi\\right)^d\\left|\\Sigma\\right|}}e^{-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_y\\right)^T\\Sigma^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_y\\right)}$\n",
    "- Parameters: $\\Sigma$ and $\\left\\{\\boldsymbol{\\mu}_k\\right\\}$\n",
    "\n",
    "Using $N_k=\\sum_i I\\left\\{y_i=k\\right\\}$\n",
    "\n",
    "Find $p_Y$ using empirical measurement:\n",
    "$$\n",
    "p_Y\\left(y=k\\right)=\\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "Find the parameters using MLE:\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{i\\ \\text{s.t.}\\ y_i=k}\\boldsymbol{x}_i\n",
    "$$\n",
    "$$\n",
    "\\Sigma = \\frac{1}{N}\\sum_{i}\\left(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_{y_i}\\right)\\left(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_{y_i}\\right)^T\n",
    "$$\n",
    "\n",
    "For misclassification rate:\n",
    "$$\n",
    "h^*\\left(\\boldsymbol{x}\\right)=\\underset{k}{\\arg\\max}\\ f_{X|Y}\\left(\\boldsymbol{x}|y=k\\right)p_Y\\left(y=k\\right)\n",
    "$$\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h^*\\left(x\\right)=\n",
    "\\begin{cases}\n",
    "1\\qquad \\boldsymbol{a}^T \\boldsymbol{x} + b > 0 \\\\\n",
    "0\\qquad \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{a}=\\Sigma^{-1}\\left(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_0\\right)$\n",
    "- $b=\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_0^T\\Sigma^{-1}\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1^T\\Sigma^{-1}\\boldsymbol{\\mu}_1\\right) + \\log\\left(\\frac{p\\left(y=1\\right)}{p\\left(y=0\\right)}\\right)$\n",
    "\n",
    "- Produces good results when the data distribution of each class can be well approximated by a Gaussian (mainly when it is concentrated in and oval like area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadric Discriminant Analysis (QDA)\n",
    "\n",
    "- Like LDA but with a different covariance matrix for each class.\n",
    "- Solves: Classification problems\n",
    "- Approach: Generative\n",
    "- Model: $f_{X|Y}=\\frac{1}{\\sqrt{\\left(2\\pi\\right)^d\\left|\\Sigma_y\\right|}}e^{-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_y\\right)^T\\Sigma^{-1}_y\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_y\\right)}$\n",
    "- Parameters: $\\left\\{\\Sigma_y\\right\\}$ and $\\left\\{\\boldsymbol{\\mu}_k\\right\\}$\n",
    "\n",
    "Using $N_k=\\sum_i I\\left\\{y_i=k\\right\\}$\n",
    "\n",
    "Find $p_Y$ using empirical measurement:\n",
    "$$\n",
    "p_Y\\left(y=k\\right)=\\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "Find the parameters using MLE:\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{i\\ \\text{s.t.}\\ y_i=k}\\boldsymbol{x}_i\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k}\\sum_{i\\ \\text{s.t.}\\ y_i=k}\\left(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_k\\right)\\left(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_k\\right)^T\n",
    "$$\n",
    "\n",
    "For misclassification rate:\n",
    "$$\n",
    "h^*\\left(\\boldsymbol{x}\\right)=\\underset{k}{\\arg\\max}\\ f_{X|Y}\\left(\\boldsymbol{x}|y=k\\right)p_Y\\left(y=k\\right)\n",
    "$$\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h^*\\left(x\\right) \n",
    "& =\n",
    "\\begin{cases}\n",
    "0\\qquad \\boldsymbol{x}^T C \\boldsymbol{x} + \\boldsymbol{a}^T \\boldsymbol{x} + b > 0 \\\\\n",
    "1\\qquad \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C=\\tfrac{1}{2}\\left(\\Sigma_0^{-1}-\\Sigma_1^{-1}\\right)$\n",
    "- $\\boldsymbol{a}=\\Sigma_1^{-1}\\boldsymbol{\\mu}_1-\\Sigma_0^{-1}\\boldsymbol{\\mu}_0$\n",
    "- $b=\\tfrac{1}{2}\\left(\\boldsymbol{\\mu}_1^T\\Sigma_1^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T\\Sigma_0^{-1}\\boldsymbol{\\mu}_0\\right) + \\log\\left(\\frac{\\left|\\Sigma_0\\right|^{1/2}p\\left(y=1\\right)}{\\left|\\Sigma_1\\right|^{1/2}p\\left(y=0\\right)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Logistic Regression\n",
    "\n",
    "- Solves: Classification problems\n",
    "- Approach: Discriminative type I\n",
    "- Model: $f_{Y|X}=\\frac{e^{\\boldsymbol{\\theta}_k^T\\boldsymbol{x}}}{\\sum_\\tilde{k}e^{\\boldsymbol{\\theta}_k^T\\boldsymbol{x}}}$\n",
    "- Parameters: $\\left\\{\\boldsymbol{\\theta}_k\\right\\}$. Without loss of generality we can set $\\boldsymbol{\\theta}_0=0$\n",
    "- Hyper-Parameters: the optimization solver parameters.\n",
    "- Solved using gradient decent\n",
    "- For binary classification: $f_{Y|X}=\\frac{1}{1+e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}}$\n",
    "\n",
    "For misclassification rate:\n",
    "$$\n",
    "h^*\\left(\\boldsymbol{x}\\right)=\\underset{k}{\\arg\\max}\\ \\boldsymbol{\\theta}_k^T\\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "- Is a generalization of LDA and works well when the data is close to being linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron (MLP)\n",
    "\n",
    "- Solves: Regression or Classification problems.\n",
    "- Approach: Discriminative type I (or Discriminative type II for regression).\n",
    "- Model: A neural network of fully connected layers.\n",
    "- Parameters: The weights of the fully connected layers\n",
    "- Hyper-Parameters: The number of layers and the width of each layer + the optimization solver parameters.\n",
    "- Solved using gradient decent (using back-propagation).\n",
    "\n",
    "- Usually tends to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "- Solves: Regression or Classification problems.\n",
    "- Approach: Discriminative type I (or Discriminative type II for regression).\n",
    "- Model: A neural network of convolutional layers and fully connected layers.\n",
    "- Parameters: The weights of the layers\n",
    "- Hyper-Parameters: The number of layers and the parameters of each layer.\n",
    "- Solved using gradient decent (using back-propagation).\n",
    "\n",
    "- Works very well when the data has some spatial structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Support Vector Machine (Hard SVM)\n",
    "\n",
    "- Solves: Binary Classification problems.\n",
    "- Approach: Discriminative type II.\n",
    "- Model: $h^*\\left(\\boldsymbol{x}\\right)=\\text{sign}\\left(\\boldsymbol{\\theta}^T\\boldsymbol{x}\\right)$\n",
    "- Parameters: $\\boldsymbol{\\theta}$\n",
    "- Solved using convex optimization methods.\n",
    "\n",
    "- Works very well when the data is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Support Vector Machine (Soft SVM)\n",
    "\n",
    "- Solves: Binary Classification problems.\n",
    "- Approach: Discriminative type II.\n",
    "- Model: $h^*\\left(\\boldsymbol{x}\\right)=\\text{sign}\\left(\\boldsymbol{\\theta}^T\\boldsymbol{x}\\right)$\n",
    "- Adds slack variables in the optimization process to deal with data which is not linearly separable.\n",
    "- Parameters: $\\boldsymbol{\\theta}$\n",
    "- Hyper-parameters: The weighting of the slack variables.\n",
    "- Solved using convex optimization methods.\n",
    "\n",
    "- Works very well when the data is close to being linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
